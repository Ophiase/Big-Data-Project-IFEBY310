[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of Citibikes - Big Data",
    "section": "",
    "text": "0.1 Installation\nPython notebook dependencies :",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Analysis of Citibikes - Big Data",
    "section": "",
    "text": "Graphics : seaborn, altair, plotly\nGeographic data : geojson, geopandas, geopy, ipyleaflet\nOthers : jupyter-lab, numpy, pandas, pyspark",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#execution",
    "href": "index.html#execution",
    "title": "Analysis of Citibikes - Big Data",
    "section": "0.2 Execution",
    "text": "0.2 Execution\nExecute the jupyter notebook report.ipynb",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#sources",
    "href": "index.html#sources",
    "title": "Analysis of Citibikes - Big Data",
    "section": "0.3 Sources",
    "text": "0.3 Sources\n\nDataset :\n\nhttps://citibikenyc.com/system-data\nhttps://api.citybik.es/citi-bike-nyc.json\nhttps://s3.amazonaws.com/tripdata/index.html\n\nOther informations :\n\nhttps://bikeshare-research.org\nhttps://citibikenyc.com\nReal time feed\nNYC Bike feeds data\nStation status feed\nBike routes\nOpenStreetMap map of New York City",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "etl.html",
    "href": "etl.html",
    "title": "2  Extract Transform and Load",
    "section": "",
    "text": "2.1 Extract\nCode\nENABLE_DOWNLOAD = False\nENABLE_UNZIP = False\nENABLE_COMPUTE_ZIP_TO_PARQUET = False\nENABLE_COMPUTE_DIMENSION = False\nENABLE_COMPUTE_EVENT = False\nENABLE_SAVE_GRAPHICS_DATA = False\nWe download the data from https://s3.amazonaws.com/tripdata/{year}-citibike-tripdata.zip\nCode\n# from 2014 to 2023\n\nzip_files = []\n\ndef download_data():\n    trip_urls = [\n        (year, f\"https://s3.amazonaws.com/tripdata/{year}-citibike-tripdata.zip\")\n        for year in range(2014, 2023 + 1)\n    ]\n\n    if not os.path.exists(os.path.join('data')) :\n        os.makedirs('data')\n\n    for year, url in trip_urls:\n        basename = os.path.join('data', str(year) + \"_\" + 'citibike_tripdata')\n        zip_filename = basename + \".zip\"\n        csv_filename = basename + \".csv\"\n        zip_files.append((year, zip_filename, csv_filename))\n\n        if not ENABLE_DOWNLOAD : continue\n        print(f'Check {basename} ...')\n\n        response = requests.get(url, stream=True)\n        total_size = int(response.headers.get('content-length', 0))\n        block_size = 1024\n        progress = 0\n\n        if not os.path.exists(zip_filename) and not os.path.exists(csv_filename) :\n            with open(zip_filename, 'wb') as f:\n                for data in response.iter_content(block_size):\n                    if data:\n                        f.write(data)\n                        progress += len(data)\n                        print(f'\\rDownloaded {progress}/{total_size} bytes', end='')\n\n            print(f'\\nDownload complete: {zip_filename}')\n\n    print(\"Finished\")\n\nif ENABLE_UNZIP or ENABLE_DOWNLOAD:\n    download_data()\nWe then unzip the data.\nCode\ndef unzip_data():\n    for (year, zip_filename, csv_filename) in zip_files:\n        # if year &lt; 2018: continue # WARNING : DISABLE THIS LINE\n\n        if not zipfile.is_zipfile(zip_filename):\n            print(\"Corrupted zip file.\")\n            break\n\n        if os.path.exists(\"tmp\"):\n            shutil.rmtree(\"tmp\")\n\n        print(\"Unzip : \", zip_filename)\n        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n            zip_ref.extractall(\"tmp\")\n        print(\"Process ..\")\n\n        # find the folder in tmp\n        items = os.listdir(\"tmp\")\n        for folder in items :\n            if not os.path.isdir(os.path.join(\"tmp\", folder)) or \\\n                folder.startswith(\"__\") :\n                continue\n            \n            # find all the folder in this folder\n            sub_folders = os.listdir(os.path.join(\"tmp\", folder))\n            for sub_folder in sub_folders :\n                if not os.path.isdir(os.path.join(\"tmp\", folder, sub_folder)) or \\\n                    sub_folder.startswith(\".\") : \n                    continue\n                \n                sub_item = os.listdir(os.path.join(\"tmp\", folder, sub_folder))\n                for leaf in sub_item :\n                    # move the csv inside to data\n                    from_path = os.path.join(\"tmp\", folder, sub_folder, leaf)\n                    dest_path = os.path.join(\"data\", leaf)\n                    if os.path.exists(dest_path) : \n                        os.remove(dest_path)\n\n                    shutil.move(from_path, \"data\")\n\n    if os.path.exists(\"tmp\"):\n        shutil.rmtree(\"tmp\")\n\nif ENABLE_UNZIP :\n    unzip_data()\nTo optimize disk usage, we could have unziped one file at a time and convert its content instantaneously to .parquet.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Extract Transform and Load</span>"
    ]
  },
  {
    "objectID": "etl.html#transform",
    "href": "etl.html#transform",
    "title": "2  Extract Transform and Load",
    "section": "2.2 Transform",
    "text": "2.2 Transform\n\n\nCode\ncsv_reader = spark.read.option(\"header\", \"true\") \\\n            .option(\"inferSchema\", \"true\").csv\n\ndef find_all_csv():\n    all_csv = []\n    for item in os.listdir(\"data\"):\n        if not item.endswith(\".csv\") :\n            continue\n        all_csv.append(item)\n    return sorted(all_csv)\n\nif False:\n    all_csv = find_all_csv()\n\nif False: # check column_names.txt\n    for item in all_csv:    \n        df = csv_reader(os.path.join(\"data\", item))\n        print(f\"item {item} : {df.columns}\")\n\n\nBy looking at the previous code output (cached in column_names.txt),\nwe notice the following columns between 2014-01 \\(\\to\\) 2021-01 (included) :\n\n['tripduration', 'starttime', 'stoptime', 'start station id', 'start station name',\n'start station latitude', 'start station longitude', 'end station id', 'end station name',\n'end station latitude', 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender']\n\nThe naming convention is not exactly the same between : 201610-citibike-tripdata_1.csv \\(\\to\\) 201703-citibike-tripdata.csv_1.csv :\n['Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID',\n'Start Station Name', 'Start Station Latitude', 'Start Station Longitude',\n'End Station ID', 'End Station Name', 'End Station Latitude',\n'End Station Longitude', 'Bike ID', 'User Type', 'Birth Year', 'Gender']\n\n\nThe columns change between 2021-02 \\(\\to\\) 2023-12 (included) :\n\n['ride_id', 'rideable_type', 'started_at', 'ended_at', 'start_station_name',\n'start_station_id', 'end_station_name', 'end_station_id', 'start_lat',\n'start_lng', 'end_lat', 'end_lng', 'member_casual']\n\nWe decide the following matching (O. as before 2021-02, N. as after 2021-02):\n\nO.'tripduration' as function : stoptime - startime\n\nRenamed as trip_duration\n\nN.'started_at' \\(\\leftarrow\\) O.'starttime'\nN.'ended_at' \\(\\leftarrow\\) O.'stoptime'\nN.'start_station_id' \\(\\leftarrow\\) O.'start station id'\n\ntype : string\n\nN.'start_station_name' \\(\\leftarrow\\) O.'start station name'\nN.'start_lat' \\(\\leftarrow\\) O.'start station latitude'\nN.'start_lng' \\(\\leftarrow\\) O.'start station longitude'\nN.'end_station_id' \\(\\leftarrow\\) O.'end station id'\n\ntype : string\n\nN.'end_station_name' \\(\\leftarrow\\) O.'end station name'\nN.'end_lat' \\(\\leftarrow\\) O.'end station latitude'\nN.'end_lng' \\(\\leftarrow\\) O.'end station longitude' `\nN.'ride_id' \\(\\leftarrow\\) O.'bikeid' (format is not the same)\n\nBoth type of ID can registered as string\n\nN.'member_casual' \\(\\leftarrow\\) O.'usertype' (format is not the same)\n\nMapping : O.Subscriber, O.Customer \\(\\to\\) N.member, N.casual\n\nO.'birth year : (None) for elements of N\n\nRenamed as birth_year\n\nO.'gender' : (None) for elements of N\nN.'rideable_type' : (None) for elements of O\nWe will also add a binary column old_format to indicate if the data comes from O or N as defined above.\n\n\n\nCode\ncol_mapping_1 = {\n    'tripduration': 'trip_duration',\n    'usertype': 'member_casual',\n    'birth year': 'birth_year',\n\n    'starttime': 'started_at',\n    'stoptime': 'ended_at',\n    'start station id': 'start_station_id',\n    'start station name': 'start_station_name',\n    'start station latitude': 'start_lat',\n    'start station longitude': 'start_lng',\n    'end station id': 'end_station_id',\n    'end station name': 'end_station_name',\n    'end station latitude': 'end_lat',\n    'end station longitude': 'end_lng',\n    'bikeid': 'ride_id',\n}\n\ncol_mapping_2 = {\n    'Trip Duration': 'tripduration',\n    'Start Time': 'starttime',\n    'Stop Time': 'stoptime',\n    \n    'Start Station ID': 'start station id',\n    'Start Station Name': 'start station name',\n    'Start Station Latitude': 'start station latitude',\n    'Start Station Longitude': 'start station longitude',\n\n    'End Station ID': 'end station id',\n    'End Station Name' : 'end station name',\n    'End Station Latitude' : 'end station latitude',\n    'End Station Longitude' : 'end station longitude',\n    \n    'Bike ID' : 'bikeid',\n    'User Type' : 'usertype',\n    'Birth Year' : 'birth year',\n    'Gender' : 'gender'\n}\n\n\n\n\nCode\ndef check_unique_values(df, column):\n    return df.select(column).dropDuplicates().rdd.map(lambda row: row[0]).collect()\n\ndef fast_check():\n    df_o = spark.read.csv(os.path.join(\"data\", all_csv[0]), header=True, inferSchema=True)\n    df_o = df_o.select(\n        [col(old_col).alias(col_mapping_1.get(old_col, old_col)) for old_col in df_o.columns]\n        )\n\n    df_n = spark.read.csv(os.path.join(\"data\", all_csv[-1]), header=True, inferSchema=True)\n    \n    print(check_unique_values(df_o, \"member_casual\"))\n    print(check_unique_values(df_o, \"gender\"))\n    print(check_unique_values(df_n, \"rideable_type\"))\n\n    df_o.printSchema()\n    df_n.printSchema()\n    \n    # df.show()\n\n# fast_check()\n\n\n\n2.2.1 File format Selection : Parquet\n\nOur course on Big Data file formats.\nParquet suits our needs for the project:\n\nSuitable for laptop execution\n37GB dataset size manageable with Parquet’s compression\nSupports splitability for processing subsets of data on minimal configuration\nCompatible with Apache Spark\n\n\n\n2.2.2 CSV \\(\\to\\) Parquet\n\n\nCode\nfactorial_columns = [\"member_casual\", \"gender\", \"rideable_type\"]\n\n# Lower Case O. part\n\n# add missing columns\ndef csv_o_process(df):\n    df = df.select(\n        [col(old_col).alias(col_mapping_1.get(old_col, old_col)) for old_col in df.columns])\n\n    df = df.withColumn(\"birth_year\", \n                       when(col(\"birth_year\") == r\"\\N\", lit(None))\n                       .otherwise(col(\"birth_year\"))\n                       .cast(\"integer\")\n                       )\n\n    df = df.withColumn(\"start_station_id\", col(\"start_station_id\").cast(\"string\"))\\\n            .withColumn(\"end_station_id\", col(\"end_station_id\").cast(\"string\")) \\\n            .withColumn(\"ended_at\", col(\"ended_at\").cast(\"timestamp\")) \\\n            .withColumn(\"started_at\", col(\"started_at\").cast(\"timestamp\")) \\\n            .withColumn(\"ride_id\", col(\"ride_id\").cast(\"string\"))\n\n    df = df.withColumn(\"member_casual\",\n                        when(col(\"member_casual\") == \"Subscriber\", lit(\"member\")) \\\n                        .otherwise(lit(\"casual\")))\n    \n    df = df.withColumn(\"rideable_type\", lit(None).cast('string'))\n    df = df.withColumn(\"old_format\", lit(True))\n    \n    df = df.withColumn(\"gender\", \n         when(col('gender') == 0, lit(None) ) \\\n        .when(col('gender') == 1, lit(\"Male\")) \\\n        .when(col('gender') == 2, lit(\"Female\")).cast(\"string\"))\n\n    df = df.select(*sorted(df.columns))\n\n    # df.write.mode(\"append\").parquet(parquet_file)\n    return df\n\ndef csv_to_parquet_part_1(csv_file) :\n    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n    return csv_o_process(df)\n\n# Upper case O. part\n\ndef csv_to_parquet_part_2(csv_file) :\n    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n    \n    df = df.select(\n        [col(old_col).alias(col_mapping_2.get(old_col, old_col)) for old_col in df.columns])\n    \n    return csv_o_process(df)\n\n# N. Part\n\ndef csv_to_parquet_part_3(csv_file) :\n    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n\n    # df = df.withColumn(\"trip_duration\", \n    #     (unix_timestamp(col(\"ended_at\")) - unix_timestamp(col(\"started_at\"))) \\\n    #     .cast(\"integer\"))\n    \n    df = df.withColumn(\"trip_duration\", \n        F.when(\n            (col(\"started_at\").isNotNull()) & (col(\"ended_at\").isNotNull()) & (col(\"ended_at\") &gt; col(\"started_at\")),\n            (F.unix_timestamp(col(\"ended_at\")) - F.unix_timestamp(col(\"started_at\"))).cast(\"integer\")\n        ).otherwise(None))\n    \n    df = df.withColumn(\"old_format\", lit(False))\n    df = df.withColumn(\"birth_year\", lit(None).cast(\"integer\"))\n\n    df = df.withColumn(\"gender\", lit(None).cast(\"string\"))\n\n    df = df.withColumn(\"start_station_id\", col(\"start_station_id\").cast(\"string\"))\\\n            .withColumn(\"end_station_id\", col(\"end_station_id\").cast(\"string\")) \\\n            .withColumn(\"ended_at\", col(\"ended_at\").cast(\"timestamp\")) \\\n            .withColumn(\"started_at\", col(\"started_at\").cast(\"timestamp\")) \\\n            .withColumn(\"ride_id\", col(\"ride_id\").cast(\"string\"))\n\n    df = df.select(*sorted(df.columns))\n\n    # df.write.mode(\"append\").parquet(parquet_file)\n    return df\n\n\nWe decide to partition columns on \"year(started_at)\".\nPartition on \"month(started_at)\" and \"start_station_id\" takes too much time.\n\n\nCode\n# Scheduler\n\nschedule = [\n    (\"201401-citibike-tripdata_1.csv\", 1),\n    (\"201610-citibike-tripdata_1.csv\", 2),\n    (\"201704-citibike-tripdata.csv_1.csv\", 1),\n    (\"202102-citibike-tripdata_1.csv\", 3)\n]\n\ndef process_csv_files(csv_files, schedule):\n    schedule_pointer = 0\n    df_buffer = []\n\n    for index, csv_file in enumerate(csv_files):\n        csv_path = os.path.join(\"data\", csv_file)\n        \n        if index % 20 == 0 :\n            print(f\"[{schedule_pointer}, {index}] {csv_file}\")\n        if (schedule_pointer &lt; len(schedule) - 1) and (csv_file == schedule[schedule_pointer + 1][0]) :\n            schedule_pointer += 1\n            print(f\"{csv_file} | {schedule_pointer} : {schedule[schedule_pointer]}\")\n\n        schedule_mode = schedule[schedule_pointer][1]\n        target_function = [csv_to_parquet_part_1, csv_to_parquet_part_2, csv_to_parquet_part_3][schedule_mode - 1]\n\n        df_to_add = target_function(csv_path)\n        df_buffer.append(df_to_add)\n\n    print (\"REDUCE STEP\")\n    \n    df = reduce(lambda df1, df2: df1.union(df2), df_buffer)\n    df = df.withColumn(\"year\",\n        when(col(\"started_at\").isNotNull(), \n                year(col(\"started_at\")).cast(\"integer\")\n            ).otherwise(lit(None).cast(\"integer\"))\n    )\n\n    print (\"WITH year/month\")\n\n    parquet_path = os.path.join(\n        'computed', 'travels', f'travels.parquet')\n\n    df \\\n        .partitionBy(\"year\") \\\n        .mode(\"overwrite\").parquet(parquet_path)\n        # .write.option(\"maxRecordsPerFile\", 70000000) \\\n\nif ENABLE_COMPUTE_ZIP_TO_PARQUET :\n    process_csv_files(all_csv, schedule)\n\n\n\n\n2.2.3 Star Schema\n\n\nCode\nif ENABLE_COMPUTE_DIMENSION:\n    df_dimension = df.select(\"start_station_id\", \"start_station_name\", \n                                    \"start_lat\", \"start_lng\") \\\n                            .withColumnRenamed(\"start_station_id\", \"station_id\") \\\n                            .withColumnRenamed(\"start_station_name\", \"station_name\") \\\n                            .withColumnRenamed(\"start_lat\", \"lat\") \\\n                            .withColumnRenamed(\"start_lng\", \"lng\")\n\n    df_dimension = df_dimension.unionByName(\n        df.select(\"end_station_id\", \"end_station_name\", \n                        \"end_lat\", \"end_lng\") \\\n                .withColumnRenamed(\"end_station_id\", \"station_id\") \\\n                .withColumnRenamed(\"end_station_name\", \"station_name\") \\\n                .withColumnRenamed(\"end_lat\", \"lat\") \\\n                .withColumnRenamed(\"end_lng\", \"lng\")\n                ).distinct()\n\n                    #, allowMissingColumns=True\n\n    df_dimension = df_dimension.groupBy(\"station_id\").agg(\n        F.first(\"station_name\").alias(\"station_name\"),\n        F.first(\"lat\").alias(\"lat\"),\n        F.first(\"lng\").alias(\"lng\")\n    )\n\n    df_dimension.explain()\n\n    df_dimension.write \\\n                .mode(\"overwrite\") \\\n                .parquet(os.path.join(\"computed\", \"dimension\", \"dimension.parquet\"))\n    #.option(\"maxRecordsPerFile\", 70000000) \\\n\n\n\n\nCode\nif ENABLE_COMPUTE_EVENT :\n    def make_event_table(df_event):\n        columns = [\n            \"start_station_name\", \"start_lng\", \"start_lat\", \n            \"end_station_name\", \"end_lng\", \"end_lat\"\n            ]\n        \n        for x in columns :\n            df_event = df_event.drop(col(x))\n\n        return df_event\n\n    df_event = make_event_table(df)\n    df_event.describe()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Extract Transform and Load</span>"
    ]
  },
  {
    "objectID": "etl.html#load",
    "href": "etl.html#load",
    "title": "2  Extract Transform and Load",
    "section": "2.3 Load",
    "text": "2.3 Load\n\n2.3.1 Full version\ndf = spark.read.parquet(os.path.join(\"computed\", \"travels\", \"*.parquet\"))\nNumber of rows : 200414362\n+----------+-----------------+------------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+----+\n|birth_year|          end_lat|           end_lng|end_station_id|    end_station_name|           ended_at|gender|member_casual|old_format|         ride_id|rideable_type|        start_lat|         start_lng|start_station_id|  start_station_name|         started_at|trip_duration|year|\n+----------+-----------------+------------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+----+\n|      NULL|        40.797911|          -73.9423|       7599.09|E 115 St & Lexing...|2023-12-30 13:24:13|  NULL|       member|     false|32BEF72918F69776| classic_bike|     40.794983625|     -73.933362842|         7450.05|Pleasant Ave & E ...|2023-12-30 13:21:45|          148|2023|\n|      NULL|         40.64741|         -73.95933|       3180.09|Albemarle Rd & E ...|2023-12-04 13:07:38|  NULL|       casual|     false|DCC39327E7923AC1| classic_bike|       40.6703837|      -73.97839676|         3905.15|        3 St & 7 Ave|2023-12-04 12:45:24|         1334|2023|\n|      NULL|         40.78839|          -73.9747|       7458.03|W 87 St & Amsterd...|2023-12-04 13:44:37|  NULL|       casual|     false|83FC5C676BF500F3| classic_bike|     40.766292214|     -73.971603751|         6904.06|     5 Ave & E 63 St|2023-12-04 13:30:36|          841|2023|\n|      NULL|       40.7859201|      -73.94860294|       7365.08|     E 97 St & 3 Ave|2023-12-13 12:55:15|  NULL|       casual|     false|6A2E79452D946A72| classic_bike|       40.7949879|       -73.9333349|         7450.05|Pleasant Ave & E ...|2023-12-13 12:43:15|          720|2023|\n|      NULL|         40.78839|          -73.9747|       7458.03|W 87 St & Amsterd...|2023-12-13 14:42:16|  NULL|       casual|     false|2E01DBEFC220A0B5| classic_bike|     40.766590834|     -73.971483469|         6904.06|     5 Ave & E 63 St|2023-12-13 14:29:45|          751|2023|\n|      NULL|         40.78839|          -73.9747|       7458.03|W 87 St & Amsterd...|2023-12-14 23:22:33|  NULL|       member|     false|7C50E5E4257B997E|electric_bike|      40.76584941|      -73.98690506|         6920.03|     W 54 St & 9 Ave|2023-12-14 23:12:01|          632|2023|\n|      NULL|       40.7859201|      -73.94860294|       7365.08|     E 97 St & 3 Ave|2023-12-20 18:29:24|  NULL|       member|     false|06DE67059C3E3D3A| classic_bike|       40.7949879|       -73.9333349|         7450.05|Pleasant Ave & E ...|2023-12-20 18:14:27|          897|2023|\n|      NULL|      40.69610226|      -73.96751037|       4724.03|Washington Ave & ...|2023-12-07 14:23:33|  NULL|       member|     false|6EE5BCD07E7B9BF3| classic_bike|40.69241829257847|-73.98949474096298|         4637.06|Fulton St & Adams St|2023-12-07 14:14:36|          537|2023|\n|      NULL|       40.8045555|       -73.9396861|       7682.01| Park Ave & E 124 St|2023-12-11 09:14:50|  NULL|       member|     false|1367A70F561EC9DE| classic_bike|     40.809318542|     -73.947855353|         7738.04|Adam Clayton Powe...|2023-12-11 09:11:36|          194|2023|\n|      NULL|        40.830663|        -73.941323|       8085.05|St Nicholas Ave &...|2023-12-16 10:19:47|  NULL|       member|     false|A45C5BF8A11C563B| classic_bike|     40.861428142|      -73.92543745|         8563.06|Nagle Ave & Thaye...|2023-12-16 10:06:11|          816|2023|\n|      NULL|40.75516719072139|-74.00059908628464|       6535.04|W 34 St & Hudson ...|2023-12-11 21:48:18|  NULL|       member|     false|C22EBED52436D69A| classic_bike|     40.745637894|     -74.004958153|         6306.01|    W 20 St & 10 Ave|2023-12-11 21:38:08|          610|2023|\n|      NULL|       40.8014866|       -73.9442507|       7640.04|E 118 St & Madiso...|2023-12-06 14:11:56|  NULL|       member|     false|A4382C9110B6D3B5| classic_bike|       40.8025566|       -73.9490782|         7655.22|Lenox Ave & W 117 St|2023-12-06 14:09:32|          144|2023|\n|      NULL|40.74965268472847| -73.9952078461647|       6373.07|     W 30 St & 8 Ave|2023-12-05 09:12:51|  NULL|       member|     false|B2BCB7C36F87784B| classic_bike|     40.756897211|     -73.999648452|         6611.07|Hudson Blvd W & W...|2023-12-05 09:08:08|          283|2023|\n|      NULL|       40.8045555|       -73.9396861|       7682.01| Park Ave & E 124 St|2023-12-08 14:07:05|  NULL|       member|     false|B5A9A77F349E283C| classic_bike|       40.7949879|       -73.9333349|         7450.05|Pleasant Ave & E ...|2023-12-08 13:55:06|          719|2023|\n|      NULL|       40.8045555|       -73.9396861|       7682.01| Park Ave & E 124 St|2023-12-11 13:14:24|  NULL|       member|     false|FE38C0F7B70B9E6B| classic_bike|     40.794885755|     -73.933385968|         7450.05|Pleasant Ave & E ...|2023-12-11 13:06:17|          487|2023|\n|      NULL|40.74965268472847| -73.9952078461647|       6373.07|     W 30 St & 8 Ave|2023-12-27 19:26:18|  NULL|       member|     false|471966F6CDB8994E| classic_bike|     40.765767097|     -73.986885786|         6920.03|     W 54 St & 9 Ave|2023-12-27 19:15:28|          650|2023|\n|      NULL|       40.8014866|       -73.9442507|       7640.04|E 118 St & Madiso...|2023-12-12 13:17:46|  NULL|       member|     false|A09F9500B615D657|electric_bike|       40.7949879|       -73.9333349|         7450.05|Pleasant Ave & E ...|2023-12-12 13:12:29|          317|2023|\n|      NULL|        40.817555|        -73.957163|       7865.09| Broadway & W 131 St|2023-12-06 08:57:37|  NULL|       member|     false|F0688F91929172B9| classic_bike|       40.8107922|       -73.9430681|         7753.13|Lenox Ave & W 130 St|2023-12-06 08:50:19|          438|2023|\n|      NULL|       40.8014866|       -73.9442507|       7640.04|E 118 St & Madiso...|2023-12-18 18:39:11|  NULL|       member|     false|8C357B52DB617F87| classic_bike|       40.8025566|       -73.9490782|         7655.22|Lenox Ave & W 117 St|2023-12-18 18:35:44|          207|2023|\n|      NULL|        40.797911|          -73.9423|       7599.09|E 115 St & Lexing...|2023-12-06 11:32:40|  NULL|       member|     false|DC1986D08A3724C1| classic_bike|       40.8025566|       -73.9490782|         7655.22|Lenox Ave & W 117 St|2023-12-06 11:28:21|          259|2023|\n+----------+-----------------+------------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+----+\n\n\n2.3.2 Star schema Version :\ndf_dimension = spark.read.parquet(os.path.join(\"computed\", \"dimension\", \"dimension.parquet\"))\ndf_event = spark.read.parquet(os.path.join(\"computed\", \"event\", \"event.parquet\"))\n\ndef build_df_star(df_event, df_dimension):\n    df_dimension_start = df_dimension.selectExpr(\n        \"station_id as start_station_id\",\n        \"station_name as start_station_name\",\n        \"lat as start_lat\",\n        \"lng as start_lng\"\n    )\n\n    df_dimension_end = df_dimension.selectExpr(\n        \"station_id as end_station_id\",\n        \"station_name as end_station_name\",\n        \"lat as end_lat\",\n        \"lng as end_lng\"\n    )\n\n    return df_event \\\n        .join(df_dimension_start, df_event.start_station_id == df_dimension_start.start_station_id, \"left\") \\\n        .join(df_dimension_end, df_event.end_station_id == df_dimension_end.end_station_id, \"left\")\n\ndf_star = build_df_star(df_event, df_dimension)\ndf_star.explain()\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- BroadcastHashJoin [end_station_id#725], [end_station_id#824], LeftOuter, BuildRight, false\n   :- BroadcastHashJoin [start_station_id#732], [start_station_id#815], LeftOuter, BuildRight, false\n   :  :- FileScan parquet [birth_year#724,end_station_id#725,ended_at#726,gender#727,member_casual#728,old_format#729,ride_id#730,rideable_type#731,start_station_id#732,started_at#733,trip_duration#734,year#735] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/astragaliton/Documents/documentation/Big-Data-Project-II-IF..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;birth_year:int,end_station_id:string,ended_at:timestamp,gender:string,member_casual:string...\n   :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=355]\n   :     +- Project [station_id#358 AS start_station_id#815, station_name#359 AS start_station_name#816, lat#360 AS start_lat#817, lng#361 AS start_lng#818]\n   :        +- Filter isnotnull(station_id#358)\n   :           +- FileScan parquet [station_id#358,station_name#359,lat#360,lng#361] Batched: true, DataFilters: [isnotnull(station_id#358)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/astragaliton/Documents/documentation/Big-Data-Project-II-IF..., PartitionFilters: [], PushedFilters: [IsNotNull(station_id)], ReadSchema: struct&lt;station_id:string,station_name:string,lat:double,lng:double&gt;\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=358]\n      +- Project [station_id#864 AS end_station_id#824, station_name#865 AS end_station_name#825, lat#866 AS end_lat#826, lng#867 AS end_lng#827]\n         +- Filter isnotnull(station_id#864)\n            +- FileScan parquet [station_id#864,station_name#865,lat#866,lng#867] Batched: true, DataFilters: [isnotnull(station_id#864)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/astragaliton/Documents/documentation/Big-Data-Project-II-IF..., PartitionFilters: [], PushedFilters: [IsNotNull(station_id)], ReadSchema: struct&lt;station_id:string,station_name:string,lat:double,lng:double&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Extract Transform and Load</span>"
    ]
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "3  Analysis and Graphics",
    "section": "",
    "text": "3.1 Trip distance by day",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "analysis.html#number-of-trips-for-each-pickupdropoff-location-couple",
    "href": "analysis.html#number-of-trips-for-each-pickupdropoff-location-couple",
    "title": "3  Analysis and Graphics",
    "section": "3.2 Number of trips for each pickup/dropoff location couple",
    "text": "3.2 Number of trips for each pickup/dropoff location couple",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "analysis.html#trip-distance-distribution-for-gender",
    "href": "analysis.html#trip-distance-distribution-for-gender",
    "title": "3  Analysis and Graphics",
    "section": "3.3 Trip distance distribution for gender",
    "text": "3.3 Trip distance distribution for gender",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "analysis.html#trip-distance-distribution-for-age-ranges",
    "href": "analysis.html#trip-distance-distribution-for-age-ranges",
    "title": "3  Analysis and Graphics",
    "section": "3.4 Trip distance distribution for age ranges",
    "text": "3.4 Trip distance distribution for age ranges",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "analysis.html#trip-distance-distribution-for-different-kind-of-bikes",
    "href": "analysis.html#trip-distance-distribution-for-different-kind-of-bikes",
    "title": "3  Analysis and Graphics",
    "section": "3.5 Trip distance distribution for different kind of bikes",
    "text": "3.5 Trip distance distribution for different kind of bikes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "analysis.html#the-number-of-pickupsdocks",
    "href": "analysis.html#the-number-of-pickupsdocks",
    "title": "3  Analysis and Graphics",
    "section": "3.6 The number of pickups/docks",
    "text": "3.6 The number of pickups/docks",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "analysis.html#the-average-distance",
    "href": "analysis.html#the-average-distance",
    "title": "3  Analysis and Graphics",
    "section": "3.7 The average distance",
    "text": "3.7 The average distance",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "analysis.html#the-average-trip-duration",
    "href": "analysis.html#the-average-trip-duration",
    "title": "3  Analysis and Graphics",
    "section": "3.8 The average trip duration",
    "text": "3.8 The average trip duration",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "analysis.html#the-average-number-of-ongoing-trips",
    "href": "analysis.html#the-average-number-of-ongoing-trips",
    "title": "3  Analysis and Graphics",
    "section": "3.9 The average number of ongoing trips",
    "text": "3.9 The average number of ongoing trips",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "analysis.html#appendix",
    "href": "analysis.html#appendix",
    "title": "3  Analysis and Graphics",
    "section": "3.10 Appendix",
    "text": "3.10 Appendix\nGraphics were made with the following code :\n\n\nCode\ndef visualize_trip_distance_over_something(metric, metric_name, df_1, df_2=None, description_1=\"\", description_2=\"\", min_factor=0.99):\n    def specific_chart(df, description_bonus):\n        min_value = df['avg_distance_km'].min() * min_factor\n        max_value = df['avg_distance_km'].max() * 1.02\n\n        return alt.Chart(df).mark_bar().encode(\n                x=alt.X(metric, title=metric_name, sort=None),\n                y=alt.Y('avg_distance_km:Q', title='Average distance (km)',\n                        scale=alt.Scale(domain=(min_value, max_value))),\n                color=alt.Color(metric, title=metric_name),\n                tooltip=['avg_distance_km', metric]\n            ).properties(\n                title= f'Average trip distance per {metric_name} {description_bonus}',\n                width= 250,\n                height= 200\n            )\n        \n\n    if df_2 is None:\n        chart = alt.hconcat(specific_chart(df_1, description_1))\n    else :\n        chart = alt.hconcat(\n            specific_chart(df_1, description_1),\n            specific_chart(df_2, description_2)\n        )\n\n    chart = chart.configure_axis(\n        labelAngle=45,\n        titleFontSize=12,\n        labelFontSize=10,\n        labelOverlap='parity'\n    ).configure_legend(\n        titleFontSize=12,\n        labelFontSize=10\n    )\n\n    return chart\n\ndef visualize_number_of_pickup_drop(ax, df, date):\n    sns.histplot(df[\"count\"], bins=30, kde=True, ax=ax)\n    ax.set_xlabel(\"Number of couple (start_station_id, end_station_id) during \" + date)\n    ax.set_ylabel(\"Number of occurrences the couple\")\n    ax.set_title(\"(start_station_id, end_station_id)\")\n\ndef visualize_top_10_pickup_dropoff(ax, top_10_df, date):\n    sns.barplot(x=\"count\", y=\"station_pair\", data=top_10_df, dodge=False, ax=ax)\n    ax.set_xlabel(\"Number of Occurrences\")\n    ax.set_ylabel(\"Station Pair\")\n    ax.set_title(\"Most Frequent Start-End Station Pairs during \" + date)\n    \n    labels = [item.get_text() for item in ax.get_yticklabels()]\n    wrapped_labels = [\"\\n\".join(label.split(\" -&gt; \")) for label in labels]\n    ax.set_yticklabels(wrapped_labels)\n\ndef full_visualization(df_reduce_pickup_dropoff, df_top_10_pickup_dropoff, date):\n    fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n    visualize_number_of_pickup_drop(axes[0], df_reduce_pickup_dropoff, date)\n    visualize_top_10_pickup_dropoff(axes[1], df_top_10_pickup_dropoff, date)\n    \n    plt.tight_layout()\n    plt.show()\ndef full_visualization_over_the_week(metric, metric_name, df_1, df_2, description_1=\"\", description_2=\"\"):\n    def specific_chart(df, description_bonus):\n        min_value = 0 # df['avg_distance_km'].min() * 0.80\n        max_value = df['avg_distance_km'].max() * 1.02\n\n        return alt.Chart(df).mark_bar().encode(\n                x=alt.X('hour_of_the_week:N', title='Hour of the week', sort=None),\n                y=alt.Y(metric, title=metric_name), #scale=alt.Scale(domain=(min_value, max_value))),\n                color=alt.Color('day_of_the_week', title='Day of the week'),\n                tooltip=[metric, 'day_of_the_week', 'hour_of_the_day']\n            ).properties(\n                title= f'{metric_name} per hour of the week ' + description_bonus,\n                width= 250,\n                height= 200\n            )\n        \n    chart = alt.hconcat(\n        specific_chart(df_1, description_1),\n        specific_chart(df_2, description_2)\n    ).configure_axis(\n        labelAngle=45\n    )\n\n    chart = chart.configure_axis(\n        titleFontSize=12,\n        labelFontSize=10,\n        labelOverlap='parity'\n    ).configure_legend(\n        titleFontSize=12,\n        labelFontSize=10\n    )\n\n    return chart\n\nfull_visualization_over_the_week_1822 = lambda x, y : full_visualization_over_the_week(x, y, data_phw_2018, data_phw_2022, \"(2018)\", \"(2022)\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analysis and Graphics</span>"
    ]
  },
  {
    "objectID": "monitoring.html",
    "href": "monitoring.html",
    "title": "4  Monitoring",
    "section": "",
    "text": "4.0.1 Relevant Job to analyze\ndf_to_analyze = spark.read.parquet(os.path.join(\"computed\", \"travels\", \"*.parquet\"))\ndf_to_analyze.show()\n\ndf_star.show()\nWe’ll also analyze some toPandas performed for visualization graphics in the notebook.\n\n\n4.0.2 Monitor Travels\ndf.explain(True)\n== Parsed Logical Plan ==\nRelation [birth_year#3175,end_lat#3176,end_lng#3177,end_station_id#3178,end_station_name#3179,ended_at#3180,gender#3181,member_casual#3182,old_format#3183,ride_id#3184,rideable_type#3185,start_lat#3186,start_lng#3187,start_station_id#3188,start_station_name#3189,started_at#3190,trip_duration#3191,year#3192] parquet\n\n== Analyzed Logical Plan ==\nbirth_year: int, end_lat: double, end_lng: double, end_station_id: string, end_station_name: string, ended_at: timestamp, gender: string, member_casual: string, old_format: boolean, ride_id: string, rideable_type: string, start_lat: double, start_lng: double, start_station_id: string, start_station_name: string, started_at: timestamp, trip_duration: int, year: int\nRelation [birth_year#3175,end_lat#3176,end_lng#3177,end_station_id#3178,end_station_name#3179,ended_at#3180,gender#3181,member_casual#3182,old_format#3183,ride_id#3184,rideable_type#3185,start_lat#3186,start_lng#3187,start_station_id#3188,start_station_name#3189,started_at#3190,trip_duration#3191,year#3192] parquet\n\n== Optimized Logical Plan ==\nRelation [birth_year#3175,end_lat#3176,end_lng#3177,end_station_id#3178,end_station_name#3179,ended_at#3180,gender#3181,member_casual#3182,old_format#3183,ride_id#3184,rideable_type#3185,start_lat#3186,start_lng#3187,start_station_id#3188,start_station_name#3189,started_at#3190,trip_duration#3191,year#3192] parquet\n\n== Physical Plan ==\n*(1) ColumnarToRow\n+- FileScan parquet [birth_year#3175,end_lat#3176,end_lng#3177,end_station_id#3178,end_station_name#3179,ended_at#3180,gender#3181,member_casual#3182,old_format#3183,ride_id#3184,rideable_type#3185,start_lat#3186,start_lng#3187,start_station_id#3188,start_station_name#3189,started_at#3190,trip_duration#3191,year#3192] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/astragaliton/Documents/documentation/Big-Data-Project-II-IF..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;birth_year:int,end_lat:double,end_lng:double,end_station_id:string,end_station_name:string...\n\nRelation does not differ between Analyzed and Optimized Logical Plan. The query optimizer in the RDBMS did not find any opportunities to further optimize the logical plan after the initial analysis phase.\nThe Physical Plan differs from the Optimized Logical Plan in that it includes the ColumnarToRow operator, which is a Spark-specific operator that converts data from the columnar format used for storage and processing to the row format used for execution. The FileScan parquet operator is also specific to Spark and is used to scan Parquet data files.\n\nIt requires :\n\nOne stage to read the parquet file of travels.\nOne stage to show it.\nTwo stages (over two jobs) to show the df_star\n\nHashAggregate efficiently performs aggregations by grouping data using hash tables, optimizing memory usage and processing speed.\nExchange hashpartitioning redistributes data across nodes based on hashed keys, enabling parallel processing and necessary data colocation for operations like joins, albeit with the cost of shuffle operations.\n\nThe above operations do not perform shuffle operations. But, neverthless, if we look down on jobs performing at .toPandas operation, they do perform a shuffle operation to do so.\n\nTasks are the smallest units of work within a stage, each processing a single data partition.\nAgain, for the 3 operations shown in this part, there is one task (except for df_star which uses 2 stages in one of its tasks). Nevertheless, most toPandas stages perform 8 task (because, the travels.parquet is partionned into the 8`** years present in the dataset)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monitoring</span>"
    ]
  },
  {
    "objectID": "spatial_informations.html",
    "href": "spatial_informations.html",
    "title": "5  Spatial Informations",
    "section": "",
    "text": "5.1 HeatMap\nCode\ndef display_heatmap(heatmap_data):\n    heatmap_data = heatmap_data.pivot(index='start_station_id', columns='end_station_id', values='trip_count')\n    return px.imshow(heatmap_data, title=\"Heatmap of Start/End stations in 2018\", labels=dict(x=\"End Station\", y=\"Start Station\", color=\"Number of Trips\"))\n\ndisplay_heatmap(heatmap_data_2018)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Informations</span>"
    ]
  },
  {
    "objectID": "spatial_informations.html#interactive-map",
    "href": "spatial_informations.html#interactive-map",
    "title": "5  Spatial Informations",
    "section": "5.2 Interactive map",
    "text": "5.2 Interactive map\n\n\nCode\ndef display_interactive_map(df, bonus=\"\"):\n    df = df.sort_values(\"hour\")\n    fig = px.density_mapbox(df, lat='lat', lon='lng', z='count', radius=30,\n                            center=dict(lat=40.7128, lon=-74.0060), zoom=10,\n                            mapbox_style=\"carto-positron\", animation_frame='hour',\n                            range_color=[0, df['count'].max()*1.0])\n\n    fig.update_layout(\n        title=\"Citibikes Station Frequency Heatmap of New York City by Hour\" + bonus,\n        updatemenus=[dict(\n            type=\"buttons\",\n            showactive=False,\n            buttons=[\n                dict(label=\"Play\",\n                     method=\"animate\",\n                     args=[None, dict(frame=dict(duration=500, redraw=True), fromcurrent=True)]),\n                dict(label=\"Pause\",\n                     method=\"animate\",\n                     args=[[None], dict(frame=dict(duration=0, redraw=False), mode=\"immediate\", fromcurrent=True)])\n            ]\n        )],\n        sliders = [dict(\n            active=0,\n            currentvalue={\"prefix\": \"Hour: \"},\n            pad={\"t\": 1}\n        )],\n        height=700, width=700\n        \n    )\n\n    fig.show(frame=0)\n\ndisplay_interactive_map(interactive_map_data_2018, \" (2018)\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Informations</span>"
    ]
  }
]