{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Project II - Big Data\n",
    "\n",
    "Author : Ophiase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_DOWNLOAD = False\n",
    "ENABLE_UNZIP = False\n",
    "ENABLE_COMPUTE_ZIP_TO_PARQUET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyspark.sql.functions as F\n",
    "import zipfile\n",
    "from functools import reduce\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--driver-memory 2g\"\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import when, mean, stddev, skewness, kurtosis, expr, date_format\n",
    "import pyspark.sql.functions as pf\n",
    "from pyspark.sql.functions import col, lit, udf, dayofweek, avg, year, month, unix_timestamp\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Big Data Project\") \\\n",
    "    .config(\"spark.driver.memory\", \"3g\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# from 2014 to 2023\n",
    "\n",
    "zip_files = []\n",
    "\n",
    "def download_data():\n",
    "    trip_urls = [\n",
    "        (year, f\"https://s3.amazonaws.com/tripdata/{year}-citibike-tripdata.zip\")\n",
    "        for year in range(2014, 2023 + 1)\n",
    "    ]\n",
    "\n",
    "    if not os.path.exists(os.path.join('data')) :\n",
    "        os.makedirs('data')\n",
    "\n",
    "    for year, url in trip_urls:\n",
    "        basename = os.path.join('data', str(year) + \"_\" + 'citibike_tripdata')\n",
    "        zip_filename = basename + \".zip\"\n",
    "        csv_filename = basename + \".csv\"\n",
    "        zip_files.append((year, zip_filename, csv_filename))\n",
    "\n",
    "        if not ENABLE_DOWNLOAD : continue\n",
    "        print(f'Check {basename} ...')\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "        progress = 0\n",
    "\n",
    "        if not os.path.exists(zip_filename) and not os.path.exists(csv_filename) :\n",
    "            with open(zip_filename, 'wb') as f:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    if data:\n",
    "                        f.write(data)\n",
    "                        progress += len(data)\n",
    "                        print(f'\\rDownloaded {progress}/{total_size} bytes', end='')\n",
    "\n",
    "            print(f'\\nDownload complete: {zip_filename}')\n",
    "\n",
    "    print(\"Finished\")\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_data():\n",
    "    for (year, zip_filename, csv_filename) in zip_files:\n",
    "        # if year < 2018: continue # WARNING : DISABLE THIS LINE\n",
    "\n",
    "        if not zipfile.is_zipfile(zip_filename):\n",
    "            print(\"Corrupted zip file.\")\n",
    "            break\n",
    "\n",
    "        if os.path.exists(\"tmp\"):\n",
    "            shutil.rmtree(\"tmp\")\n",
    "\n",
    "        print(\"Unzip : \", zip_filename)\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"tmp\")\n",
    "        print(\"Process ..\")\n",
    "\n",
    "        # find the folder in tmp\n",
    "        items = os.listdir(\"tmp\")\n",
    "        for folder in items :\n",
    "            if not os.path.isdir(os.path.join(\"tmp\", folder)) or \\\n",
    "                folder.startswith(\"__\") :\n",
    "                continue\n",
    "            \n",
    "            # find all the folder in this folder\n",
    "            sub_folders = os.listdir(os.path.join(\"tmp\", folder))\n",
    "            for sub_folder in sub_folders :\n",
    "                if not os.path.isdir(os.path.join(\"tmp\", folder, sub_folder)) or \\\n",
    "                    sub_folder.startswith(\".\") : \n",
    "                    continue\n",
    "                \n",
    "                sub_item = os.listdir(os.path.join(\"tmp\", folder, sub_folder))\n",
    "                for leaf in sub_item :\n",
    "                    # move the csv inside to data\n",
    "                    from_path = os.path.join(\"tmp\", folder, sub_folder, leaf)\n",
    "                    dest_path = os.path.join(\"data\", leaf)\n",
    "                    if os.path.exists(dest_path) : \n",
    "                        os.remove(dest_path)\n",
    "\n",
    "                    shutil.move(from_path, \"data\")\n",
    "\n",
    "    if os.path.exists(\"tmp\"):\n",
    "        shutil.rmtree(\"tmp\")\n",
    "\n",
    "if ENABLE_UNZIP :\n",
    "    unzip_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize disk usage, we could have unziped one file at a time and convert its content instantaneously to `.parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader = spark.read.option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\").csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_csv():\n",
    "    all_csv = []\n",
    "    for item in os.listdir(\"data\"):\n",
    "        if not item.endswith(\".csv\") :\n",
    "            continue\n",
    "        all_csv.append(item)\n",
    "    return sorted(all_csv)\n",
    "\n",
    "all_csv = find_all_csv()\n",
    "\n",
    "if False: # check column_names.txt\n",
    "    for item in all_csv:    \n",
    "        df = csv_reader(os.path.join(\"data\", item))\n",
    "        print(f\"item {item} : {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the previous code output *(cached in `column_names.txt`)*, \\\n",
    "we notice the following columns between 2014-01 $\\to$ 2021-01 (included) :\n",
    "- `['tripduration', 'starttime', 'stoptime', 'start station id', 'start station name', `\\\n",
    "`'start station latitude', 'start station longitude', 'end station id', 'end station name', `\\\n",
    "`'end station latitude', 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender']`\n",
    "    - The naming convention is not exactly the same between : `201610-citibike-tripdata_1.csv` $\\to$ `201703-citibike-tripdata.csv_1.csv` : \\\n",
    "    `['Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID',` \\\n",
    "    `'Start Station Name', 'Start Station Latitude', 'Start Station Longitude',` \\\n",
    "    `'End Station ID', 'End Station Name', 'End Station Latitude',` \\\n",
    "    `'End Station Longitude', 'Bike ID', 'User Type', 'Birth Year', 'Gender']`\n",
    "\n",
    "The columns change between 2021-02 $\\to$ 2023-12 (included) :\n",
    "- `['ride_id', 'rideable_type', 'started_at', 'ended_at', 'start_station_name', `\\\n",
    "`'start_station_id', 'end_station_name', 'end_station_id', 'start_lat', `\\\n",
    "`'start_lng', 'end_lat', 'end_lng', 'member_casual']`\n",
    "\n",
    "We decide the following matching (**O.** as before 2021-02, **N.** as after 2021-02):\n",
    "\n",
    "- `O.'tripduration'` as function : `stoptime` - `startime`\n",
    "    - Renamed as `trip_duration`\n",
    "\n",
    "- `N.'started_at'` $\\leftarrow$ `O.'starttime'`\n",
    "- `N.'ended_at'` $\\leftarrow$ `O.'stoptime'` \n",
    "- `N.'start_station_id'` $\\leftarrow$ `O.'start station id'`\n",
    "    - type : string\n",
    "- `N.'start_station_name'` $\\leftarrow$ `O.'start station name'`\n",
    "- `N.'start_lat'` $\\leftarrow$ `O.'start station latitude'`\n",
    "- `N.'start_lng'` $\\leftarrow$ `O.'start station longitude'`\n",
    "- `N.'end_station_id'` $\\leftarrow$ `O.'end station id'`\n",
    "    - type : string\n",
    "- `N.'end_station_name'` $\\leftarrow$ `O.'end station name'`\n",
    "- `N.'end_lat'` $\\leftarrow$ `O.'end station latitude'`\n",
    "- `N.'end_lng'` $\\leftarrow$ `O.'end station longitude'`\n",
    "`\n",
    "- `N.'ride_id'` $\\leftarrow$ `O.'bikeid'` (format is not the same)\n",
    "    - Both type of ID can registered as string\n",
    "\n",
    "- `N.'member_casual'` $\\leftarrow$ `O.'usertype'` (format is not the same)\n",
    "    - Mapping : `O.Subscriber`, `O.Customer` $\\to$ `N.member`, `N.casual`\n",
    "\n",
    "- `O.'birth year` : (None) for elements of N\n",
    "    - Renamed as `birth_year`\n",
    "- `O.'gender'` : (None) for elements of N\n",
    "- `N.'rideable_type'` : (None) for elements of O\n",
    "\n",
    "- We will also add a binary column `old_format` to indicate if the data comes from `O` or `N` as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mapping_1 = {\n",
    "    'tripduration': 'trip_duration',\n",
    "    'usertype': 'member_casual',\n",
    "    'birth year': 'birth_year',\n",
    "\n",
    "    'starttime': 'started_at',\n",
    "    'stoptime': 'ended_at',\n",
    "    'start station id': 'start_station_id',\n",
    "    'start station name': 'start_station_name',\n",
    "    'start station latitude': 'start_lat',\n",
    "    'start station longitude': 'start_lng',\n",
    "    'end station id': 'end_station_id',\n",
    "    'end station name': 'end_station_name',\n",
    "    'end station latitude': 'end_lat',\n",
    "    'end station longitude': 'end_lng',\n",
    "    'bikeid': 'ride_id',\n",
    "}\n",
    "\n",
    "col_mapping_2 = {\n",
    "    'Trip Duration': 'tripduration',\n",
    "    'Start Time': 'starttime',\n",
    "    'Stop Time': 'stoptime',\n",
    "    \n",
    "    'Start Station ID': 'start station id',\n",
    "    'Start Station Name': 'start station name',\n",
    "    'Start Station Latitude': 'start station latitude',\n",
    "    'Start Station Longitude': 'start station longitude',\n",
    "\n",
    "    'End Station ID': 'end station id',\n",
    "    'End Station Name' : 'end station name',\n",
    "    'End Station Latitude' : 'end station latitude',\n",
    "    'End Station Longitude' : 'end station longitude',\n",
    "    \n",
    "    'Bike ID' : 'bikeid',\n",
    "    'User Type' : 'usertype',\n",
    "    'Birth Year' : 'birth year',\n",
    "    'Gender' : 'gender'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df, column):\n",
    "    return df.select(column).dropDuplicates().rdd.map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subscriber', 'Customer']\n",
      "[1, 2, 0]\n",
      "['electric_bike', 'classic_bike']\n",
      "root\n",
      " |-- trip_duration: integer (nullable = true)\n",
      " |-- started_at: timestamp (nullable = true)\n",
      " |-- ended_at: timestamp (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_lat: double (nullable = true)\n",
      " |-- start_lng: double (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_lat: double (nullable = true)\n",
      " |-- end_lng: double (nullable = true)\n",
      " |-- ride_id: integer (nullable = true)\n",
      " |-- member_casual: string (nullable = true)\n",
      " |-- birth_year: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ride_id: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- started_at: timestamp (nullable = true)\n",
      " |-- ended_at: timestamp (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- start_lat: double (nullable = true)\n",
      " |-- start_lng: double (nullable = true)\n",
      " |-- end_lat: double (nullable = true)\n",
      " |-- end_lng: double (nullable = true)\n",
      " |-- member_casual: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fast_check():\n",
    "    df_o = spark.read.csv(os.path.join(\"data\", all_csv[0]), header=True, inferSchema=True)\n",
    "    df_o = df_o.select(\n",
    "        [col(old_col).alias(col_mapping_1.get(old_col, old_col)) for old_col in df_o.columns]\n",
    "        )\n",
    "\n",
    "    df_n = spark.read.csv(os.path.join(\"data\", all_csv[-1]), header=True, inferSchema=True)\n",
    "    \n",
    "    print(check_unique_values(df_o, \"member_casual\"))\n",
    "    print(check_unique_values(df_o, \"gender\"))\n",
    "    print(check_unique_values(df_n, \"rideable_type\"))\n",
    "\n",
    "    df_o.printSchema()\n",
    "    df_n.printSchema()\n",
    "    \n",
    "    # df.show()\n",
    "\n",
    "fast_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File format Selection : Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/file-formats.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Our [course](https://stephane-v-boucheron.fr/slides/tbd/slides06_file-formats.html#/title-slide) on Big Data file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet suits our needs for the project:\n",
    "\n",
    "- Suitable for laptop execution\n",
    "- 37GB dataset size manageable with Parquet's compression\n",
    "- Supports splitability for processing subsets of data on minimal configuration\n",
    "- Compatible with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV $\\to$ Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('computed')) :\n",
    "    os.makedirs('computed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial_columns = [\"member_casual\", \"gender\", \"rideable_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Case O. part\n",
    "\n",
    "# add missing columns\n",
    "def csv_o_process(df):\n",
    "    df = df.select(\n",
    "        [col(old_col).alias(col_mapping_1.get(old_col, old_col)) for old_col in df.columns])\n",
    "\n",
    "    df = df.withColumn(\"birth_year\", \n",
    "                       when(col(\"birth_year\") == r\"\\N\", lit(None))\n",
    "                       .otherwise(col(\"birth_year\"))\n",
    "                       .cast(\"integer\")\n",
    "                       )\n",
    "\n",
    "    df = df.withColumn(\"start_station_id\", col(\"start_station_id\").cast(\"string\"))\\\n",
    "            .withColumn(\"end_station_id\", col(\"end_station_id\").cast(\"string\")) \\\n",
    "            .withColumn(\"ended_at\", col(\"ended_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"started_at\", col(\"started_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"ride_id\", col(\"ride_id\").cast(\"string\"))\n",
    "\n",
    "    df = df.withColumn(\"member_casual\",\n",
    "                        when(col(\"member_casual\") == \"Subscriber\", lit(\"member\")) \\\n",
    "                        .otherwise(lit(\"casual\")))\n",
    "    \n",
    "    df = df.withColumn(\"rideable_type\", lit(None).cast('string'))\n",
    "    df = df.withColumn(\"old_format\", lit(True))\n",
    "    \n",
    "    df = df.withColumn(\"gender\", \n",
    "         when(col('gender') == 0, lit(None) ) \\\n",
    "        .when(col('gender') == 1, lit(\"Male\")) \\\n",
    "        .when(col('gender') == 2, lit(\"Female\")).cast(\"string\"))\n",
    "\n",
    "    df = df.select(*sorted(df.columns))\n",
    "\n",
    "    # df.write.mode(\"append\").parquet(parquet_file)\n",
    "    return df\n",
    "\n",
    "def csv_to_parquet_part_1(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    return csv_o_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper case O. part\n",
    "\n",
    "def csv_to_parquet_part_2(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    \n",
    "    df = df.select(\n",
    "        [col(old_col).alias(col_mapping_2.get(old_col, old_col)) for old_col in df.columns])\n",
    "    \n",
    "    return csv_o_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N. Part\n",
    "\n",
    "def csv_to_parquet_part_3(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "\n",
    "    # df = df.withColumn(\"trip_duration\", \n",
    "    #     (unix_timestamp(col(\"ended_at\")) - unix_timestamp(col(\"started_at\"))) \\\n",
    "    #     .cast(\"integer\"))\n",
    "    \n",
    "    df = df.withColumn(\"trip_duration\", \n",
    "        F.when(\n",
    "            (col(\"started_at\").isNotNull()) & (col(\"ended_at\").isNotNull()) & (col(\"ended_at\") > col(\"started_at\")),\n",
    "            (F.unix_timestamp(col(\"ended_at\")) - F.unix_timestamp(col(\"started_at\"))).cast(\"integer\")\n",
    "        ).otherwise(None))\n",
    "    \n",
    "    df = df.withColumn(\"old_format\", lit(False))\n",
    "    df = df.withColumn(\"birth_year\", lit(None).cast(\"integer\"))\n",
    "\n",
    "    df = df.withColumn(\"gender\", lit(None).cast(\"string\"))\n",
    "\n",
    "    df = df.withColumn(\"start_station_id\", col(\"start_station_id\").cast(\"string\"))\\\n",
    "            .withColumn(\"end_station_id\", col(\"end_station_id\").cast(\"string\")) \\\n",
    "            .withColumn(\"ended_at\", col(\"ended_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"started_at\", col(\"started_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"ride_id\", col(\"ride_id\").cast(\"string\"))\n",
    "\n",
    "    df = df.select(*sorted(df.columns))\n",
    "\n",
    "    # df.write.mode(\"append\").parquet(parquet_file)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Partitioning\n",
    "\n",
    "We decide to partition columns on `\"year(started_at)\"`, `\"month(started_at)\"`, `\"start_station_id\"`\n",
    "\n",
    "Even with Lazy Loading of the CSV file, dataset doesn't seems to fit in RAM, thus we partition by hand the .parquet file intos multiples files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler\n",
    "\n",
    "schedule = [\n",
    "    (\"201401-citibike-tripdata_1.csv\", 1),\n",
    "    (\"201610-citibike-tripdata_1.csv\", 2),\n",
    "    (\"201704-citibike-tripdata.csv_1.csv\", 1),\n",
    "    (\"202102-citibike-tripdata_1.csv\", 3)\n",
    "]\n",
    "\n",
    "def process_csv_files(csv_files, schedule):\n",
    "    schedule_pointer = 0\n",
    "    df_buffer = []\n",
    "\n",
    "    for index, csv_file in enumerate(csv_files):\n",
    "        csv_path = os.path.join(\"data\", csv_file)\n",
    "        \n",
    "        if index % 20 == 0 :\n",
    "            print(f\"[{schedule_pointer}, {index}] {csv_file}\")\n",
    "        if (schedule_pointer < len(schedule) - 1) and (csv_file == schedule[schedule_pointer + 1][0]) :\n",
    "            schedule_pointer += 1\n",
    "            print(f\"{csv_file} | {schedule_pointer} : {schedule[schedule_pointer]}\")\n",
    "\n",
    "        schedule_mode = schedule[schedule_pointer][1]\n",
    "        target_function = [csv_to_parquet_part_1, csv_to_parquet_part_2, csv_to_parquet_part_3][schedule_mode - 1]\n",
    "\n",
    "        df_to_add = target_function(csv_path)\n",
    "        df_buffer.append(df_to_add)\n",
    "\n",
    "    print (\"REDUCE STEP\")\n",
    "    \n",
    "    df = reduce(lambda df1, df2: df1.union(df2), df_buffer)\n",
    "    df = df.withColumn(\"year\",\n",
    "        when(col(\"started_at\").isNotNull(), \n",
    "                year(col(\"started_at\")).cast(\"integer\")\n",
    "            ).otherwise(lit(None).cast(\"integer\"))\n",
    "    )\n",
    "\n",
    "    print (\"WITH year/month\")\n",
    "\n",
    "    parquet_path = os.path.join(\n",
    "        'computed', 'precompiled', f'citibike.parquet')\n",
    "\n",
    "    df \\\n",
    "        .partitionBy(\"year\") \\\n",
    "        .mode(\"overwrite\").parquet(parquet_path)\n",
    "        # .write.option(\"maxRecordsPerFile\", 70000000) \\\n",
    "\n",
    "if ENABLE_COMPUTE_ZIP_TO_PARQUET :\n",
    "    process_csv_files(all_csv, schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Parquet is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecompiled_ter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rows : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(df\u001b[38;5;241m.\u001b[39mcount()))\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(os.path.join(\"computed\", \"precompiled_ter\", \"*.parquet\"))\n",
    "\n",
    "print(\"Number of rows : \" + str(df.count()))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[station_id#592593, station_name#592598, lat#592603, lng#592608], functions=[])\n",
      "   +- Exchange hashpartitioning(station_id#592593, station_name#592598, lat#592603, lng#592608, 200), ENSURE_REQUIREMENTS, [plan_id=65605]\n",
      "      +- HashAggregate(keys=[station_id#592593, station_name#592598, knownfloatingpointnormalized(normalizenanandzero(lat#592603)) AS lat#592603, knownfloatingpointnormalized(normalizenanandzero(lng#592608)) AS lng#592608], functions=[])\n",
      "         +- Union\n",
      "            :- Project [start_station_id#591492 AS station_id#592593, start_station_name#591493 AS station_name#592598, start_lat#591490 AS lat#592603, start_lng#591491 AS lng#592608]\n",
      "            :  +- FileScan parquet [start_lat#591490,start_lng#591491,start_station_id#591492,start_station_name#591493,year#591496] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/astragaliton/Documents/documentation/Big-Data-Project-II-IF..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<start_lat:double,start_lng:double,start_station_id:string,start_station_name:string>\n",
      "            +- Project [end_station_id#592640 AS station_id#592617, end_station_name#592641 AS station_name#592622, end_lat#592638 AS lat#592627, end_lng#592639 AS lng#592632]\n",
      "               +- FileScan parquet [end_lat#592638,end_lng#592639,end_station_id#592640,end_station_name#592641,year#592654] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/astragaliton/Documents/documentation/Big-Data-Project-II-IF..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<end_lat:double,end_lng:double,end_station_id:string,end_station_name:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_dim = df.select(\"start_station_id\", \"start_station_name\", \n",
    "                                \"start_lat\", \"start_lng\") \\\n",
    "                        .withColumnRenamed(\"start_station_id\", \"station_id\") \\\n",
    "                        .withColumnRenamed(\"start_station_name\", \"station_name\") \\\n",
    "                        .withColumnRenamed(\"start_lat\", \"lat\") \\\n",
    "                        .withColumnRenamed(\"start_lng\", \"lng\")\n",
    "\n",
    "stations_dim = stations_dim.unionByName(\n",
    "    df.select(\"end_station_id\", \"end_station_name\", \n",
    "                    \"end_lat\", \"end_lng\") \\\n",
    "             .withColumnRenamed(\"end_station_id\", \"station_id\") \\\n",
    "             .withColumnRenamed(\"end_station_name\", \"station_name\") \\\n",
    "             .withColumnRenamed(\"end_lat\", \"lat\") \\\n",
    "             .withColumnRenamed(\"end_lng\", \"lng\")\n",
    "            ).distinct()\n",
    "\n",
    "                #, allowMissingColumns=True\n",
    "\n",
    "stations_dim.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 03:35:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:35:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:35:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:35:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:35:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:35:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/astragaliton/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/astragaliton/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/astragaliton/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m stations_dim\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m----> 3\u001b[0m             \u001b[38;5;241m.\u001b[39mparquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstations_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstations_dim.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 03:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:36:26 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:36:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:36:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/22 03:36:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 4425:======>                                                (2 + 8) / 16]\r"
     ]
    }
   ],
   "source": [
    "stations_dim.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .parquet(os.path.join(\"computed\", \"stations_dim\", \"stations_dim.parquet\"))\n",
    "\n",
    "#.option(\"maxRecordsPerFile\", 70000000) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date_pre_covid = '2020-02-29'\n",
    "start_date_post_covid = '2020-03-01'\n",
    "\n",
    "df_pre_covid = df.filter(col('ended_at') <= end_date_pre_covid)\n",
    "df_2018 = df.filter((col('ended_at') <= '2019-01-01') & (col('started_at') > '2018-01-01')) \n",
    "\n",
    "df_post_covid = df.filter(col('started_at') >= start_date_post_covid)\n",
    "df_2022 = df.filter((col('ended_at') <= '2023-01-01') & (col('started_at') > '2022-01-01')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Covid-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 594:==================================================>    (83 + 7) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+--------------+--------------------+--------------------+------+-------------+----------+-------+-------------+------------------+------------------+----------------+--------------------+--------------------+-------------+\n",
      "|birth_year|    end_lat|     end_lng|end_station_id|    end_station_name|            ended_at|gender|member_casual|old_format|ride_id|rideable_type|         start_lat|         start_lng|start_station_id|  start_station_name|          started_at|trip_duration|\n",
      "+----------+-----------+------------+--------------+--------------------+--------------------+------+-------------+----------+-------+-------------+------------------+------------------+----------------+--------------------+--------------------+-------------+\n",
      "|      1993|40.71850211|-73.98329859|         349.0|Rivington St & Ri...|2018-12-20 14:12:...|  Male|       member|      true|  34649|         NULL|       40.72580614|      -73.97422494|           339.0|  Avenue D & E 12 St|2018-12-20 14:01:...|          642|\n",
      "|      1984| 40.7937704|  -73.971888|        3314.0|  W 95 St & Broadway|2018-12-20 14:06:...|Female|       member|      true|  27153|         NULL|          40.78839|          -73.9747|          3285.0|W 87 St & Amsterd...|2018-12-20 14:01:...|          285|\n",
      "|      1953| 40.6769694|   -73.96579|        3574.0|Prospect Pl & Und...|2018-12-20 14:12:...|Female|       member|      true|  17947|         NULL|40.675146838709786|-73.97523209452629|          3346.0| Berkeley Pl & 7 Ave|2018-12-20 14:01:...|          619|\n",
      "|      1988|40.71605866|-73.99190759|         361.0|Allen St & Hester St|2018-12-20 14:14:...|  Male|       member|      true|  29757|         NULL|       40.72710258|      -74.00297088|           128.0|MacDougal St & Pr...|2018-12-20 14:01:...|          761|\n",
      "|      1961|40.74691959|-74.00451887|         462.0|    W 22 St & 10 Ave|2018-12-20 14:06:...|  Male|       member|      true|  33800|         NULL|       40.74345335|      -74.00004031|           470.0|     W 20 St & 8 Ave|2018-12-20 14:01:...|          297|\n",
      "+----------+-----------+------------+--------------+--------------------+--------------------+------+-------------+----------+-------+-------------+------------------+------------------+----------------+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Before Covid-19\")\n",
    "df_pre_covid.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Covid-19\n",
      "+----------+-----------------+-----------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+\n",
      "|birth_year|          end_lat|          end_lng|end_station_id|    end_station_name|           ended_at|gender|member_casual|old_format|         ride_id|rideable_type|        start_lat|         start_lng|start_station_id|  start_station_name|         started_at|trip_duration|\n",
      "+----------+-----------------+-----------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+\n",
      "|      NULL|40.72970805644994|-73.9865979552269|       5746.02|     E 10 St & 2 Ave|2023-09-03 10:24:16|  NULL|       member|     false|B0A0F1DEFA4B72FC|electric_bike|40.72486122254819|-73.99213135242462|         5636.13|     E 1 St & Bowery|2023-09-03 10:20:41|         NULL|\n",
      "|      NULL|      40.72019576|     -73.98997825|       5414.06|Allen St & Riving...|2023-09-27 15:53:25|  NULL|       member|     false|2B26AB15647BF4EE| classic_bike|     40.704650044|     -74.009132624|         4993.02|Pearl St & Hanove...|2023-09-27 15:44:23|         NULL|\n",
      "|      NULL|40.72970805644994|-73.9865979552269|       5746.02|     E 10 St & 2 Ave|2023-09-19 13:48:11|  NULL|       member|     false|9D2B5971CA4E513F| classic_bike|     40.724753261|     -73.992115974|         5636.13|     E 1 St & Bowery|2023-09-19 13:40:48|         NULL|\n",
      "|      NULL|      40.72317958|     -73.99480012|       5561.04| Mott St & Prince St|2023-09-30 16:56:35|  NULL|       member|     false|17E6760596DC3ABE| classic_bike|     40.696706414|     -73.922934651|         4713.01|Central Ave & Him...|2023-09-30 16:27:50|         NULL|\n",
      "|      NULL|      40.72317958|     -73.99480012|       5561.04| Mott St & Prince St|2023-09-21 17:07:36|  NULL|       member|     false|97EFF376A7E2DC70| classic_bike|       40.7284186|      -73.98713956|         5669.10| St Marks Pl & 2 Ave|2023-09-21 16:59:53|         NULL|\n",
      "+----------+-----------------+-----------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"After Covid-19\")\n",
    "df_post_covid.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Rayon de la Terre en kilom√®tres\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compte_trip_distance(df, verbose=False):\n",
    "    if verbose : print(\"compute distance\")\n",
    "\n",
    "    df = df.filter(\n",
    "        col(\"start_lat\").isNotNull() & \n",
    "        col(\"start_lng\").isNotNull() & \n",
    "        col(\"end_lat\").isNotNull() & \n",
    "        col(\"end_lng\").isNotNull()\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"distance_km\", haversine_udf(col(\"start_lat\"), \n",
    "                                     col(\"start_lng\"), \n",
    "                                     col(\"end_lat\"), \n",
    "                                     col(\"end_lng\")))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_2018 = compte_trip_distance(df_2018)\n",
    "df_2022 = compte_trip_distance(df_2022)\n",
    "df_post_covid = compte_trip_distance(df_post_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip distance by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trip_distance_over_week(df, verbose=False):\n",
    "    '''\n",
    "        Returns in the pandas format\n",
    "    '''\n",
    "    if verbose : print(\"compute day of week\")\n",
    "    df = df.withColumn(\"day_of_the_week\", dayofweek(col(\"started_at\")))\n",
    "\n",
    "    if verbose : print(\"group by\")\n",
    "    \n",
    "    avg_distance_by_day_of_the_week = df.groupBy(\"day_of_the_week\") \\\n",
    "        .agg(avg(\"distance_km\").alias(\"avg_distance_km\"))\n",
    "    \n",
    "    avg_distance_by_day_of_the_week = avg_distance_by_day_of_the_week.withColumn(\n",
    "        \"day_of_the_week\",\n",
    "        when(col(\"day_of_the_week\") == 1, \"Sunday\")\n",
    "        .when(col(\"day_of_the_week\") == 2, \"Monday\")\n",
    "        .when(col(\"day_of_the_week\") == 3, \"Tuesday\")\n",
    "        .when(col(\"day_of_the_week\") == 4, \"Wednesday\")\n",
    "        .when(col(\"day_of_the_week\") == 5, \"Thursday\")\n",
    "        .when(col(\"day_of_the_week\") == 6, \"Friday\")\n",
    "        .when(col(\"day_of_the_week\") == 7, \"Saturday\")\n",
    "    )\n",
    "\n",
    "    return avg_distance_by_day_of_the_week.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip_distance_over_week_2018 = compute_trip_distance_over_week(df_2018)\n",
    "trip_distance_over_week_2022 = compute_trip_distance_over_week(df_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-c5053e4e1cb1468d85fe692be63d0770.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-c5053e4e1cb1468d85fe692be63d0770.vega-embed details,\n",
       "  #altair-viz-c5053e4e1cb1468d85fe692be63d0770.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-c5053e4e1cb1468d85fe692be63d0770\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-c5053e4e1cb1468d85fe692be63d0770\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-c5053e4e1cb1468d85fe692be63d0770\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.8.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.8.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 10, \"labelOverlap\": \"parity\", \"titleFontSize\": 12}, \"legend\": {\"labelFontSize\": 10, \"titleFontSize\": 12}}, \"hconcat\": [{\"data\": {\"name\": \"data-682e8ec785edbb7251655d352f8c942d\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"day_of_the_week\", \"title\": \"Day of The Week\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"avg_distance_km\", \"type\": \"quantitative\"}, {\"field\": \"day_of_the_week\", \"type\": \"nominal\"}], \"x\": {\"field\": \"day_of_the_week\", \"sort\": null, \"title\": \"Day\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"avg_distance_km\", \"scale\": {\"domain\": [1.7286987316528177, 1.8375700115233708]}, \"title\": \"Average distance (km)\", \"type\": \"quantitative\"}}, \"height\": 200, \"name\": \"view_1\", \"title\": \"Average trip distance per Day(2018)\", \"width\": 300}, {\"data\": {\"name\": \"data-4cc96c16257368b6241f3ee2a60bb4e2\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"day_of_the_week\", \"title\": \"Day of The Week\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"avg_distance_km\", \"type\": \"quantitative\"}, {\"field\": \"day_of_the_week\", \"type\": \"nominal\"}], \"x\": {\"field\": \"day_of_the_week\", \"sort\": null, \"title\": \"Day\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"avg_distance_km\", \"scale\": {\"domain\": [1.7776798013767485, 1.946188640045717]}, \"title\": \"Average distance (km)\", \"type\": \"quantitative\"}}, \"height\": 200, \"name\": \"view_2\", \"title\": \"Average trip distance per Day(2022)\", \"width\": 300}], \"params\": [{\"name\": \"param_1\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_1\", \"view_2\"]}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.8.0.json\", \"datasets\": {\"data-682e8ec785edbb7251655d352f8c942d\": [{\"day_of_the_week\": \"Sunday\", \"avg_distance_km\": 1.7461603350028463}, {\"day_of_the_week\": \"Friday\", \"avg_distance_km\": 1.7766569111752961}, {\"day_of_the_week\": \"Tuesday\", \"avg_distance_km\": 1.7983723666188365}, {\"day_of_the_week\": \"Thursday\", \"avg_distance_km\": 1.7957771868050352}, {\"day_of_the_week\": \"Wednesday\", \"avg_distance_km\": 1.801539226983697}, {\"day_of_the_week\": \"Saturday\", \"avg_distance_km\": 1.7828920533192325}, {\"day_of_the_week\": \"Monday\", \"avg_distance_km\": 1.7913233909698545}], \"data-4cc96c16257368b6241f3ee2a60bb4e2\": [{\"day_of_the_week\": \"Sunday\", \"avg_distance_km\": 1.875066031662407}, {\"day_of_the_week\": \"Friday\", \"avg_distance_km\": 1.811983727789995}, {\"day_of_the_week\": \"Tuesday\", \"avg_distance_km\": 1.8201703663119906}, {\"day_of_the_week\": \"Thursday\", \"avg_distance_km\": 1.8254055881989864}, {\"day_of_the_week\": \"Wednesday\", \"avg_distance_km\": 1.8318979345853759}, {\"day_of_the_week\": \"Saturday\", \"avg_distance_km\": 1.908028078476193}, {\"day_of_the_week\": \"Monday\", \"avg_distance_km\": 1.7956361630068167}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_trip_distance_over_week(df_1, df_2, description_1=\"\", description_2=\"\"):\n",
    "    def specific_chart(df, description_bonus):\n",
    "        min_value = df['avg_distance_km'].min() * 0.99\n",
    "        max_value = df['avg_distance_km'].max() * 1.02\n",
    "\n",
    "        return alt.Chart(df).mark_bar().encode(\n",
    "                x=alt.X('day_of_the_week:O', title='Day', sort=None),\n",
    "                y=alt.Y('avg_distance_km:Q', title='Average distance (km)',\n",
    "                        scale=alt.Scale(domain=(min_value, max_value))),\n",
    "                color=alt.Color('day_of_the_week:N', title='Day of The Week'),\n",
    "                tooltip=['avg_distance_km', 'day_of_the_week']\n",
    "            ).properties(\n",
    "                title= 'Average trip distance per Day' + description_bonus,\n",
    "                width= 300,\n",
    "                height= 200\n",
    "            )\n",
    "        \n",
    "    chart = alt.hconcat(\n",
    "        specific_chart(df_1, description_1),\n",
    "        specific_chart(df_2, description_2)\n",
    "    ).configure_axis(\n",
    "        labelAngle=45\n",
    "    )\n",
    "\n",
    "    chart = chart.configure_axis(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10,\n",
    "        labelOverlap='parity'\n",
    "    ).configure_legend(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10\n",
    "    )\n",
    "\n",
    "    return chart\n",
    "\n",
    "visualize_trip_distance_over_week(\n",
    "    trip_distance_over_week_2018, trip_distance_over_week_2022, \n",
    "    \"(2018)\", \"(2022)\"\n",
    "    ).interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Number of trips for each pickup/dropoff location couple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip distance distribution for gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trip_distance_over_genders(df, verbose=False):\n",
    "    '''\n",
    "        Returns in Pandas format\n",
    "    '''\n",
    "    df = df.filter(col(\"gender\").isNotNull())\n",
    "    \n",
    "    avg_distance = df.groupBy(\"gender\") \\\n",
    "        .agg(avg(\"distance_km\").alias(\"avg_distance_km\"))\n",
    "    \n",
    "    return avg_distance.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip_distance_over_genders_2018 = compute_trip_distance_over_genders(df_2018)\n",
    "trip_distance_over_genders_post_covid = compute_trip_distance_over_genders(df_post_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-90bb9cd48a5344cc94172659f672ae11.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-90bb9cd48a5344cc94172659f672ae11.vega-embed details,\n",
       "  #altair-viz-90bb9cd48a5344cc94172659f672ae11.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-90bb9cd48a5344cc94172659f672ae11\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-90bb9cd48a5344cc94172659f672ae11\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-90bb9cd48a5344cc94172659f672ae11\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.8.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.8.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 10, \"labelOverlap\": \"parity\", \"titleFontSize\": 12}, \"legend\": {\"labelFontSize\": 10, \"titleFontSize\": 12}}, \"hconcat\": [{\"data\": {\"name\": \"data-90a20d581e368179c3370a40fb377586\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"gender\", \"title\": \"Gender\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"avg_distance_km\", \"type\": \"quantitative\"}, {\"field\": \"gender\", \"type\": \"nominal\"}], \"x\": {\"field\": \"gender\", \"sort\": null, \"title\": \"Gender\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"avg_distance_km\", \"scale\": {\"domain\": [0, 1.8753888129089031]}, \"title\": \"Average distance (km)\", \"type\": \"quantitative\"}}, \"height\": 200, \"name\": \"view_5\", \"title\": \"Average trip distance per Gender (2018)\", \"width\": 300}, {\"data\": {\"name\": \"data-2aabe4f339f9cdef729fe7fcf953e31b\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"gender\", \"title\": \"Gender\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"avg_distance_km\", \"type\": \"quantitative\"}, {\"field\": \"gender\", \"type\": \"nominal\"}], \"x\": {\"field\": \"gender\", \"sort\": null, \"title\": \"Gender\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"avg_distance_km\", \"scale\": {\"domain\": [0, 2.208690639559422]}, \"title\": \"Average distance (km)\", \"type\": \"quantitative\"}}, \"height\": 200, \"name\": \"view_6\", \"title\": \"Average trip distance per Gender (post covid)\", \"width\": 300}], \"params\": [{\"name\": \"param_3\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_5\", \"view_6\"]}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.8.0.json\", \"datasets\": {\"data-90a20d581e368179c3370a40fb377586\": [{\"gender\": \"Female\", \"avg_distance_km\": 1.8386164832440226}, {\"gender\": \"Male\", \"avg_distance_km\": 1.752309030450661}], \"data-2aabe4f339f9cdef729fe7fcf953e31b\": [{\"gender\": \"Female\", \"avg_distance_km\": 2.1653829799602176}, {\"gender\": \"Male\", \"avg_distance_km\": 2.0282100144660213}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_trip_distance_over_genders(df_1, df_2, description_1=\"\", description_2=\"\"):\n",
    "    def specific_chart(df, description_bonus):\n",
    "        min_value = 0 # df['avg_distance_km'].min() * 0.99\n",
    "        max_value = df['avg_distance_km'].max() * 1.02\n",
    "\n",
    "        return alt.Chart(df).mark_bar().encode(\n",
    "                x=alt.X('gender:O', title='Gender', sort=None),\n",
    "                y=alt.Y('avg_distance_km:Q', title='Average distance (km)',\n",
    "                        scale=alt.Scale(domain=(min_value, max_value))),\n",
    "                color=alt.Color('gender', title='Gender'),\n",
    "                tooltip=['avg_distance_km', 'gender']\n",
    "            ).properties(\n",
    "                title= 'Average trip distance per Gender' + description_bonus,\n",
    "                width= 300,\n",
    "                height= 200\n",
    "            )\n",
    "        \n",
    "    chart = alt.hconcat(\n",
    "        specific_chart(df_1, description_1),\n",
    "        specific_chart(df_2, description_2)\n",
    "    ).configure_axis(\n",
    "        labelAngle=45\n",
    "    )\n",
    "\n",
    "    chart = chart.configure_axis(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10,\n",
    "        labelOverlap='parity'\n",
    "    ).configure_legend(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10\n",
    "    )\n",
    "\n",
    "    return chart\n",
    "\n",
    "visualize_trip_distance_over_genders(\n",
    "    trip_distance_over_genders_2018, trip_distance_over_genders_post_covid,\n",
    "    \" (2018)\", \" (post covid)\"\n",
    "    ).interactive()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip distance distribution for age ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trip_distance_over_ages(df, verbose=False):\n",
    "    '''\n",
    "        Returns in Pandas format\n",
    "    '''\n",
    "        \n",
    "    df = df.filter(col(\"birth_year\").isNotNull())\n",
    "    df = df.withColumn(\"age\", year(col('started_at')) - col('birth_year'))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"age_range\", \n",
    "         when(col('age') <= 14, lit(None) ) \\\n",
    "        .when((15 <= col('age')) & (col('age') <= 24), lit('15-24')) \\\n",
    "        .when((25 <= col('age')) & (col('age') <= 44), lit('25-45')) \\\n",
    "        .when((45 <= col('age')) & (col('age') <= 54), lit('45-54')) \\\n",
    "        .when((55 <= col('age')) & (col('age') <= 64), lit('55-64')) \\\n",
    "        .when(65 <= col('age'), lit('65+')) \\\n",
    "    )\n",
    "\n",
    "    avg_distance = df.groupBy(\"age_range\") \\\n",
    "        .agg(avg(\"distance_km\").alias(\"avg_distance_km\"))\n",
    "\n",
    "    return avg_distance.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip_distance_over_ages_2018 = compute_trip_distance_over_ages(df_2018)\n",
    "trip_distance_over_ages_post_covid = compute_trip_distance_over_ages(df_post_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-5ff8485209394f098086cd30a5c03072.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-5ff8485209394f098086cd30a5c03072.vega-embed details,\n",
       "  #altair-viz-5ff8485209394f098086cd30a5c03072.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-5ff8485209394f098086cd30a5c03072\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-5ff8485209394f098086cd30a5c03072\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-5ff8485209394f098086cd30a5c03072\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.8.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.8.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 10, \"labelOverlap\": \"parity\", \"titleFontSize\": 12}, \"legend\": {\"labelFontSize\": 10, \"titleFontSize\": 12}}, \"hconcat\": [{\"data\": {\"name\": \"data-9c6b62e6c834cf83d0f0905694d0f989\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"age_range\", \"title\": \"Age Range\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"avg_distance_km\", \"type\": \"quantitative\"}, {\"field\": \"age_range\", \"type\": \"nominal\"}], \"x\": {\"field\": \"age_range\", \"sort\": null, \"title\": \"Age Range\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"avg_distance_km\", \"scale\": {\"domain\": [0, 1.8469391555077095]}, \"title\": \"Average distance (km)\", \"type\": \"quantitative\"}}, \"height\": 200, \"name\": \"view_11\", \"title\": \"Average trip distance per age range (2018) \", \"width\": 300}, {\"data\": {\"name\": \"data-27b123d2681b19173b00026b3a3fa728\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"age_range\", \"title\": \"Age Range\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"avg_distance_km\", \"type\": \"quantitative\"}, {\"field\": \"age_range\", \"type\": \"nominal\"}], \"x\": {\"field\": \"age_range\", \"sort\": null, \"title\": \"Age Range\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"avg_distance_km\", \"scale\": {\"domain\": [0, 2.2090825127688825]}, \"title\": \"Average distance (km)\", \"type\": \"quantitative\"}}, \"height\": 200, \"name\": \"view_12\", \"title\": \"Average trip distance per age range (post covid) \", \"width\": 300}], \"params\": [{\"name\": \"param_6\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_11\", \"view_12\"]}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.8.0.json\", \"datasets\": {\"data-9c6b62e6c834cf83d0f0905694d0f989\": [{\"age_range\": \"45-54\", \"avg_distance_km\": 1.8107246622624602}, {\"age_range\": \"25-45\", \"avg_distance_km\": 1.803088142818274}, {\"age_range\": \"55-64\", \"avg_distance_km\": 1.7145375289090745}, {\"age_range\": \"15-24\", \"avg_distance_km\": 1.7260052309493565}, {\"age_range\": \"65+\", \"avg_distance_km\": 1.5704035944342858}], \"data-27b123d2681b19173b00026b3a3fa728\": [{\"age_range\": \"45-54\", \"avg_distance_km\": 2.068127397068098}, {\"age_range\": \"25-45\", \"avg_distance_km\": 2.1657671693812572}, {\"age_range\": \"55-64\", \"avg_distance_km\": 1.8572959176065265}, {\"age_range\": \"15-24\", \"avg_distance_km\": 2.020331552182135}, {\"age_range\": \"65+\", \"avg_distance_km\": 1.6570905966112621}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_trip_distance_over_ages(df_1, df_2, description_1=\"\", description_2=\"\"):\n",
    "    def specific_chart(df, description_bonus):\n",
    "        min_value = 0 # df['avg_distance_km'].min() * 0.80\n",
    "        max_value = df['avg_distance_km'].max() * 1.02\n",
    "\n",
    "        return alt.Chart(df).mark_bar().encode(\n",
    "                x=alt.X('age_range:O', title='Age Range', sort=None),\n",
    "                y=alt.Y('avg_distance_km:Q', title='Average distance (km)',\n",
    "                        scale=alt.Scale(domain=(min_value, max_value))),\n",
    "                color=alt.Color('age_range', title='Age Range'),\n",
    "                tooltip=['avg_distance_km', 'age_range']\n",
    "            ).properties(\n",
    "                title= 'Average trip distance per age range' + description_bonus,\n",
    "                width= 300,\n",
    "                height= 200\n",
    "            )\n",
    "        \n",
    "    chart = alt.hconcat(\n",
    "        specific_chart(df_1, description_1),\n",
    "        specific_chart(df_2, description_2)\n",
    "    ).configure_axis(\n",
    "        labelAngle=45\n",
    "    )\n",
    "\n",
    "    chart = chart.configure_axis(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10,\n",
    "        labelOverlap='parity'\n",
    "    ).configure_legend(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10\n",
    "    )\n",
    "\n",
    "    return chart\n",
    "\n",
    "visualize_trip_distance_over_ages(\n",
    "    trip_distance_over_ages_2018, trip_distance_over_ages_post_covid,\n",
    "    \" (2018) \", \" (post covid) \"\n",
    "    ).interactive()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip distance distribution for different kind of bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trip_distance_over_bikes(df, verbose=False):\n",
    "    df = df.filter(col(\"rideable_type\").isNotNull())\n",
    "    \n",
    "    avg_distance = df.groupBy(\"rideable_type\") \\\n",
    "        .agg(avg(\"distance_km\").alias(\"avg_distance_km\"))\n",
    "\n",
    "    return avg_distance.toPandas\n",
    "\n",
    "trip_distance_over_bikes_2022 = compute_trip_distance_over_bikes(df_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trip_distance_over_bikes(df_1):\n",
    "    min_value = 0 # df['avg_distance_km'].min() * 0.80\n",
    "    max_value = df['avg_distance_km'].max() * 1.02\n",
    "\n",
    "    chart = alt.Chart(df).mark_bar().encode(\n",
    "            x=alt.X('rideable_type:O', title='Age Range', sort=None),\n",
    "            y=alt.Y('avg_distance_km:Q', title='Average distance (km)',\n",
    "                    scale=alt.Scale(domain=(min_value, max_value))),\n",
    "            color=alt.Color('rideable_type', title='Type of bike'),\n",
    "            tooltip=['avg_distance_km', 'age_range']\n",
    "        ).properties(\n",
    "            title= 'Average trip distance per type of types of bike after covid',\n",
    "            width= 300,\n",
    "            height= 200\n",
    "        )\n",
    "        \n",
    "    chart = chart.configure_axis(\n",
    "        labelAngle=45,\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10,\n",
    "        labelOverlap='parity'\n",
    "    ).configure_legend(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10\n",
    "    )\n",
    "\n",
    "    return chart\n",
    "\n",
    "visualize_trip_distance_over_bikes(\n",
    "    trip_distance_over_bikes_post_covid,\n",
    "    ).interactive()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hour of the week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of pickups/docks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average trip duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average number of ongoing trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation [birth_year#591479,end_lat#591480,end_lng#591481,end_station_id#591482,end_station_name#591483,ended_at#591484,gender#591485,member_casual#591486,old_format#591487,ride_id#591488,rideable_type#591489,start_lat#591490,start_lng#591491,start_station_id#591492,start_station_name#591493,started_at#591494,trip_duration#591495,year#591496] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "birth_year: int, end_lat: double, end_lng: double, end_station_id: string, end_station_name: string, ended_at: timestamp, gender: string, member_casual: string, old_format: boolean, ride_id: string, rideable_type: string, start_lat: double, start_lng: double, start_station_id: string, start_station_name: string, started_at: timestamp, trip_duration: int, year: int\n",
      "Relation [birth_year#591479,end_lat#591480,end_lng#591481,end_station_id#591482,end_station_name#591483,ended_at#591484,gender#591485,member_casual#591486,old_format#591487,ride_id#591488,rideable_type#591489,start_lat#591490,start_lng#591491,start_station_id#591492,start_station_name#591493,started_at#591494,trip_duration#591495,year#591496] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [birth_year#591479,end_lat#591480,end_lng#591481,end_station_id#591482,end_station_name#591483,ended_at#591484,gender#591485,member_casual#591486,old_format#591487,ride_id#591488,rideable_type#591489,start_lat#591490,start_lng#591491,start_station_id#591492,start_station_name#591493,started_at#591494,trip_duration#591495,year#591496] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [birth_year#591479,end_lat#591480,end_lng#591481,end_station_id#591482,end_station_name#591483,ended_at#591484,gender#591485,member_casual#591486,old_format#591487,ride_id#591488,rideable_type#591489,start_lat#591490,start_lng#591491,start_station_id#591492,start_station_name#591493,started_at#591494,trip_duration#591495,year#591496] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/astragaliton/Documents/documentation/Big-Data-Project-II-IF..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<birth_year:int,end_lat:double,end_lng:double,end_station_id:string,end_station_name:string...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2] Relation doesn't differ between Analyzed and Optimized Logical Plan. The query optimizer in the RDBMS did not find any opportunities to further optimize the logical plan after the initial analysis phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3] The Physical Plan differs from the Optimized Logical Plan in that it includes the ColumnarToRow operator, which is a Spark-specific operator that converts data from the columnar format used for storage and processing to the row format used for execution. The FileScan parquet operator is also specific to Spark and is used to scan Parquet data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4] TODO The provided code snippet does not include any information about stages or the Spark UI, so I cannot determine how many stages are necessary to complete the Spark job. HashAggregate and Exchange hashpartitioning are used for aggregation and shuffling data between partitions, respectively, but their roles cannot be inferred from the given information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5] TODO: The Physical Plan shows a single FileScan parquet operator, which suggests that no shuffle operations are performed. However, without more context, I cannot definitively say whether shuffle operations are performed elsewhere in the query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6] TODO: In Spark, tasks are the individual units of execution within a stage. The number of tasks in a stage depends on the number of partitions of the input data. However, without information about the input data and partitioning, I cannot determine the number of tasks in the stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Informations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
