{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Citibikes - Big Data\n",
    "\n",
    "Author : Ophiase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the first execution those options should all be set to True.\n",
    "# Once those operations are performed, you can rely on the baked version and disable them.\n",
    "#\n",
    "# If you already have the https://s3.amazonaws.com/tripdata/{year}-citibike-tripdata.zip\n",
    "# You can directly disable ENABLE_DOWNLOAD and put the zip files into data\n",
    "#\n",
    "# For the graphics, we do not use the star schema. \n",
    "# Thus if you disable \"DO_NOT_IGNORE_STAR_SCHEMA\", \n",
    "# then \"ENABLE_COMPUTER\" and \"ENABLE_COMPUTE_EVENT\" will be ignored.\n",
    "\n",
    "# EXTRACT\n",
    "ENABLE_DOWNLOAD = True\n",
    "ENABLE_UNZIP = True\n",
    "\n",
    "# TRANSFORM\n",
    "ENABLE_COMPUTE_ZIP_TO_PARQUET = True\n",
    "DO_NOT_IGNORE_STAR_SCHEMA = True\n",
    "ENABLE_COMPUTE_DIMENSION = True\n",
    "ENABLE_COMPUTE_EVENT = True\n",
    "\n",
    "# AGGREGATE\n",
    "ENABLE_COMPUTE_GRAPHICS_DATA = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import zipfile\n",
    "from functools import reduce\n",
    "import math\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--driver-memory 2g\"\n",
    "\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when, mean, stddev, skewness, kurtosis, expr, date_format, count\n",
    "from pyspark.sql.functions import col, desc, lit, udf, dayofweek, avg, year, month, unix_timestamp, hour\n",
    "\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Big_Data_Project\") \\\n",
    "    .config(\"spark.driver.memory\", \"3g\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 2014 to 2023\n",
    "\n",
    "zip_files = []\n",
    "\n",
    "def download_data():\n",
    "    trip_urls = [\n",
    "        (year, f\"https://s3.amazonaws.com/tripdata/{year}-citibike-tripdata.zip\")\n",
    "        for year in range(2014, 2023 + 1)\n",
    "    ]\n",
    "\n",
    "    if not os.path.exists(os.path.join('data')) :\n",
    "        os.makedirs('data')\n",
    "\n",
    "    for year, url in trip_urls:\n",
    "        basename = os.path.join('data', str(year) + \"_\" + 'citibike_tripdata')\n",
    "        zip_filename = basename + \".zip\"\n",
    "        csv_filename = basename + \".csv\"\n",
    "        zip_files.append((year, zip_filename, csv_filename))\n",
    "\n",
    "        if not ENABLE_DOWNLOAD : continue\n",
    "        print(f'Check {basename} ...')\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "        progress = 0\n",
    "\n",
    "        if not os.path.exists(zip_filename) and not os.path.exists(csv_filename) :\n",
    "            with open(zip_filename, 'wb') as f:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    if data:\n",
    "                        f.write(data)\n",
    "                        progress += len(data)\n",
    "                        print(f'\\rDownloaded {progress}/{total_size} bytes', end='')\n",
    "\n",
    "            print(f'\\nDownload complete: {zip_filename}')\n",
    "\n",
    "    print(\"Finished\")\n",
    "\n",
    "if ENABLE_UNZIP or ENABLE_DOWNLOAD:\n",
    "    download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_data():\n",
    "    for (year, zip_filename, csv_filename) in zip_files:\n",
    "        # if year < 2018: continue # WARNING : DISABLE THIS LINE\n",
    "\n",
    "        if not zipfile.is_zipfile(zip_filename):\n",
    "            print(\"Corrupted zip file.\")\n",
    "            break\n",
    "\n",
    "        if os.path.exists(\"tmp\"):\n",
    "            shutil.rmtree(\"tmp\")\n",
    "\n",
    "        print(\"Unzip : \", zip_filename)\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"tmp\")\n",
    "        print(\"Process ..\")\n",
    "\n",
    "        # find the folder in tmp\n",
    "        items = os.listdir(\"tmp\")\n",
    "        for folder in items :\n",
    "            if not os.path.isdir(os.path.join(\"tmp\", folder)) or \\\n",
    "                folder.startswith(\"__\") :\n",
    "                continue\n",
    "            \n",
    "            # find all the folder in this folder\n",
    "            sub_folders = os.listdir(os.path.join(\"tmp\", folder))\n",
    "            for sub_folder in sub_folders :\n",
    "                if not os.path.isdir(os.path.join(\"tmp\", folder, sub_folder)) or \\\n",
    "                    sub_folder.startswith(\".\") : \n",
    "                    continue\n",
    "                \n",
    "                sub_item = os.listdir(os.path.join(\"tmp\", folder, sub_folder))\n",
    "                for leaf in sub_item :\n",
    "                    # move the csv inside to data\n",
    "                    from_path = os.path.join(\"tmp\", folder, sub_folder, leaf)\n",
    "                    dest_path = os.path.join(\"data\", leaf)\n",
    "                    if os.path.exists(dest_path) : \n",
    "                        os.remove(dest_path)\n",
    "\n",
    "                    shutil.move(from_path, \"data\")\n",
    "\n",
    "    if os.path.exists(\"tmp\"):\n",
    "        shutil.rmtree(\"tmp\")\n",
    "\n",
    "if ENABLE_UNZIP :\n",
    "    unzip_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize disk usage, we could have unziped one file at a time and convert its content instantaneously to `.parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning / Convert the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical/Raw Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader = spark.read.option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\").csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_csv():\n",
    "    all_csv = []\n",
    "    for item in os.listdir(\"data\"):\n",
    "        if not item.endswith(\".csv\") :\n",
    "            continue\n",
    "        all_csv.append(item)\n",
    "    return sorted(all_csv)\n",
    "\n",
    "all_csv = find_all_csv()\n",
    "\n",
    "if False: # check column_names.txt\n",
    "    for item in all_csv:    \n",
    "        df = csv_reader(os.path.join(\"data\", item))\n",
    "        print(f\"item {item} : {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the previous code output *(cached in `column_names.txt`)*, \\\n",
    "we notice the following columns between 2014-01 $\\to$ 2021-01 (included) :\n",
    "- `['tripduration', 'starttime', 'stoptime', 'start station id', 'start station name', `\\\n",
    "`'start station latitude', 'start station longitude', 'end station id', 'end station name', `\\\n",
    "`'end station latitude', 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender']`\n",
    "    - The naming convention is not exactly the same between : `201610-citibike-tripdata_1.csv` $\\to$ `201703-citibike-tripdata.csv_1.csv` : \\\n",
    "    `['Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID',` \\\n",
    "    `'Start Station Name', 'Start Station Latitude', 'Start Station Longitude',` \\\n",
    "    `'End Station ID', 'End Station Name', 'End Station Latitude',` \\\n",
    "    `'End Station Longitude', 'Bike ID', 'User Type', 'Birth Year', 'Gender']`\n",
    "\n",
    "The columns change between 2021-02 $\\to$ 2023-12 (included) :\n",
    "- `['ride_id', 'rideable_type', 'started_at', 'ended_at', 'start_station_name', `\\\n",
    "`'start_station_id', 'end_station_name', 'end_station_id', 'start_lat', `\\\n",
    "`'start_lng', 'end_lat', 'end_lng', 'member_casual']`\n",
    "\n",
    "We decide the following matching (**O.** as before 2021-02, **N.** as after 2021-02):\n",
    "\n",
    "- `O.'tripduration'` as function : `stoptime` - `startime`\n",
    "    - Renamed as `trip_duration`\n",
    "\n",
    "- `N.'started_at'` $\\leftarrow$ `O.'starttime'`\n",
    "- `N.'ended_at'` $\\leftarrow$ `O.'stoptime'` \n",
    "- `N.'start_station_id'` $\\leftarrow$ `O.'start station id'`\n",
    "    - type : string\n",
    "- `N.'start_station_name'` $\\leftarrow$ `O.'start station name'`\n",
    "- `N.'start_lat'` $\\leftarrow$ `O.'start station latitude'`\n",
    "- `N.'start_lng'` $\\leftarrow$ `O.'start station longitude'`\n",
    "- `N.'end_station_id'` $\\leftarrow$ `O.'end station id'`\n",
    "    - type : string\n",
    "- `N.'end_station_name'` $\\leftarrow$ `O.'end station name'`\n",
    "- `N.'end_lat'` $\\leftarrow$ `O.'end station latitude'`\n",
    "- `N.'end_lng'` $\\leftarrow$ `O.'end station longitude'`\n",
    "`\n",
    "- `N.'ride_id'` $\\leftarrow$ `O.'bikeid'` (format is not the same)\n",
    "    - Both type of ID can registered as string\n",
    "\n",
    "- `N.'member_casual'` $\\leftarrow$ `O.'usertype'` (format is not the same)\n",
    "    - Mapping : `O.Subscriber`, `O.Customer` $\\to$ `N.member`, `N.casual`\n",
    "\n",
    "- `O.'birth year` : (None) for elements of N\n",
    "    - Renamed as `birth_year`\n",
    "- `O.'gender'` : (None) for elements of N\n",
    "- `N.'rideable_type'` : (None) for elements of O\n",
    "\n",
    "- We will also add a binary column `old_format` to indicate if the data comes from `O` or `N` as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mapping_1 = {\n",
    "    'tripduration': 'trip_duration',\n",
    "    'usertype': 'member_casual',\n",
    "    'birth year': 'birth_year',\n",
    "\n",
    "    'starttime': 'started_at',\n",
    "    'stoptime': 'ended_at',\n",
    "    'start station id': 'start_station_id',\n",
    "    'start station name': 'start_station_name',\n",
    "    'start station latitude': 'start_lat',\n",
    "    'start station longitude': 'start_lng',\n",
    "    'end station id': 'end_station_id',\n",
    "    'end station name': 'end_station_name',\n",
    "    'end station latitude': 'end_lat',\n",
    "    'end station longitude': 'end_lng',\n",
    "    'bikeid': 'ride_id',\n",
    "}\n",
    "\n",
    "col_mapping_2 = {\n",
    "    'Trip Duration': 'tripduration',\n",
    "    'Start Time': 'starttime',\n",
    "    'Stop Time': 'stoptime',\n",
    "    \n",
    "    'Start Station ID': 'start station id',\n",
    "    'Start Station Name': 'start station name',\n",
    "    'Start Station Latitude': 'start station latitude',\n",
    "    'Start Station Longitude': 'start station longitude',\n",
    "\n",
    "    'End Station ID': 'end station id',\n",
    "    'End Station Name' : 'end station name',\n",
    "    'End Station Latitude' : 'end station latitude',\n",
    "    'End Station Longitude' : 'end station longitude',\n",
    "    \n",
    "    'Bike ID' : 'bikeid',\n",
    "    'User Type' : 'usertype',\n",
    "    'Birth Year' : 'birth year',\n",
    "    'Gender' : 'gender'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df, column):\n",
    "    return df.select(column).dropDuplicates().rdd.map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_check():\n",
    "    df_o = spark.read.csv(os.path.join(\"data\", all_csv[0]), header=True, inferSchema=True)\n",
    "    df_o = df_o.select(\n",
    "        [col(old_col).alias(col_mapping_1.get(old_col, old_col)) for old_col in df_o.columns]\n",
    "        )\n",
    "\n",
    "    df_n = spark.read.csv(os.path.join(\"data\", all_csv[-1]), header=True, inferSchema=True)\n",
    "    \n",
    "    print(check_unique_values(df_o, \"member_casual\"))\n",
    "    print(check_unique_values(df_o, \"gender\"))\n",
    "    print(check_unique_values(df_n, \"rideable_type\"))\n",
    "\n",
    "    df_o.printSchema()\n",
    "    df_n.printSchema()\n",
    "    \n",
    "    # df.show()\n",
    "\n",
    "fast_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File format Selection : Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/file-formats.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Our [course](https://stephane-v-boucheron.fr/slides/tbd/slides06_file-formats.html#/title-slide) on Big Data file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet suits our needs for the project:\n",
    "\n",
    "- Suitable for laptop execution\n",
    "- 37GB dataset size manageable with Parquet's compression\n",
    "- Supports splitability for processing subsets of data on minimal configuration\n",
    "- Compatible with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV $\\to$ Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('computed')) :\n",
    "    os.makedirs('computed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial_columns = [\"member_casual\", \"gender\", \"rideable_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Case O. part\n",
    "\n",
    "# add missing columns\n",
    "def csv_o_process(df):\n",
    "    df = df.select(\n",
    "        [col(old_col).alias(col_mapping_1.get(old_col, old_col)) for old_col in df.columns])\n",
    "\n",
    "    df = df.withColumn(\"birth_year\", \n",
    "                       when(col(\"birth_year\") == r\"\\N\", lit(None))\n",
    "                       .otherwise(col(\"birth_year\"))\n",
    "                       .cast(\"integer\")\n",
    "                       )\n",
    "\n",
    "    df = df.withColumn(\"start_station_id\", col(\"start_station_id\").cast(\"string\"))\\\n",
    "            .withColumn(\"end_station_id\", col(\"end_station_id\").cast(\"string\")) \\\n",
    "            .withColumn(\"ended_at\", col(\"ended_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"started_at\", col(\"started_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"ride_id\", col(\"ride_id\").cast(\"string\"))\n",
    "\n",
    "    df = df.withColumn(\"member_casual\",\n",
    "                        when(col(\"member_casual\") == \"Subscriber\", lit(\"member\")) \\\n",
    "                        .otherwise(lit(\"casual\")))\n",
    "    \n",
    "    df = df.withColumn(\"rideable_type\", lit(None).cast('string'))\n",
    "    df = df.withColumn(\"old_format\", lit(True))\n",
    "    \n",
    "    df = df.withColumn(\"gender\", \n",
    "         when(col('gender') == 0, lit(None) ) \\\n",
    "        .when(col('gender') == 1, lit(\"Male\")) \\\n",
    "        .when(col('gender') == 2, lit(\"Female\")).cast(\"string\"))\n",
    "\n",
    "    df = df.select(*sorted(df.columns))\n",
    "\n",
    "    # df.write.mode(\"append\").parquet(parquet_file)\n",
    "    return df\n",
    "\n",
    "def csv_to_parquet_part_1(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    return csv_o_process(df)\n",
    "\n",
    "# Lower case O. part (with wrong the date format)\n",
    "def csv_to_parquet_part_4(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    df = df.withColumn(\"starttime\", F.to_timestamp(df.starttime, \"M/d/yyyy H:mm:ss\")) \\\n",
    "       .withColumn(\"stoptime\", F.to_timestamp(df.stoptime, \"M/d/yyyy H:mm:ss\"))\n",
    "    return csv_o_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper case O. part\n",
    "\n",
    "def csv_to_parquet_part_2(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    \n",
    "    df = df.select(\n",
    "        [col(old_col).alias(col_mapping_2.get(old_col, old_col)) for old_col in df.columns])\n",
    "    \n",
    "    return csv_o_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N. Part\n",
    "\n",
    "def csv_to_parquet_part_3(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "\n",
    "    # df = df.withColumn(\"trip_duration\", \n",
    "    #     (unix_timestamp(col(\"ended_at\")) - unix_timestamp(col(\"started_at\"))) \\\n",
    "    #     .cast(\"integer\"))\n",
    "    \n",
    "    df = df.withColumn(\"trip_duration\", \n",
    "        F.when(\n",
    "            (col(\"started_at\").isNotNull()) & (col(\"ended_at\").isNotNull()) & (col(\"ended_at\") > col(\"started_at\")),\n",
    "            (F.unix_timestamp(col(\"ended_at\")) - F.unix_timestamp(col(\"started_at\"))).cast(\"integer\")\n",
    "        ).otherwise(None))\n",
    "    \n",
    "    df = df.withColumn(\"old_format\", lit(False))\n",
    "    df = df.withColumn(\"birth_year\", lit(None).cast(\"integer\"))\n",
    "\n",
    "    df = df.withColumn(\"gender\", lit(None).cast(\"string\"))\n",
    "\n",
    "    df = df.withColumn(\"start_station_id\", col(\"start_station_id\").cast(\"string\"))\\\n",
    "            .withColumn(\"end_station_id\", col(\"end_station_id\").cast(\"string\")) \\\n",
    "            .withColumn(\"ended_at\", col(\"ended_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"started_at\", col(\"started_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"ride_id\", col(\"ride_id\").cast(\"string\"))\n",
    "\n",
    "    df = df.select(*sorted(df.columns))\n",
    "\n",
    "    # df.write.mode(\"append\").parquet(parquet_file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Partitioning\n",
    "\n",
    "We decide to partition columns on `\"year(started_at)\"`, `\"month(started_at)\"`, `\"start_station_id\"`\n",
    "\n",
    "Even with Lazy Loading of the CSV file, dataset doesn't seems to fit in RAM, thus we partition by hand the .parquet file intos multiples files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler\n",
    "\n",
    "schedule = [\n",
    "    (\"201401-citibike-tripdata_1.csv\", 1),\n",
    "    (\"201409-citibike-tripdata_1.csv\", 4),\n",
    "    (\"201610-citibike-tripdata_1.csv\", 2),\n",
    "    (\"201704-citibike-tripdata.csv_1.csv\", 1),\n",
    "    (\"202102-citibike-tripdata_1.csv\", 3)\n",
    "]\n",
    "\n",
    "def process_csv_files(csv_files, schedule):\n",
    "    schedule_pointer = 0\n",
    "    df_buffer = []\n",
    "\n",
    "    for index, csv_file in enumerate(csv_files):\n",
    "        csv_path = os.path.join(\"data\", csv_file)\n",
    "        \n",
    "        if index % 20 == 0 :\n",
    "            print(f\"[{schedule_pointer}, {index}] {csv_file}\")\n",
    "        if (schedule_pointer < len(schedule) - 1) and (csv_file == schedule[schedule_pointer + 1][0]) :\n",
    "            schedule_pointer += 1\n",
    "            print(f\"{csv_file} | {schedule_pointer} : {schedule[schedule_pointer]}\")\n",
    "\n",
    "        schedule_mode = schedule[schedule_pointer][1]\n",
    "        target_function = [csv_to_parquet_part_1, csv_to_parquet_part_2, csv_to_parquet_part_3, csv_to_parquet_part_4][schedule_mode - 1]\n",
    "\n",
    "        df_to_add = target_function(csv_path)\n",
    "        df_buffer.append(df_to_add)\n",
    "\n",
    "    print (\"REDUCE STEP\")\n",
    "    \n",
    "    df = reduce(lambda df1, df2: df1.union(df2), df_buffer)\n",
    "    df = df.withColumn(\"year\",\n",
    "        when(col(\"started_at\").isNotNull(), \n",
    "                year(col(\"started_at\")).cast(\"integer\")\n",
    "            ).otherwise(lit(None).cast(\"integer\"))\n",
    "    )\n",
    "\n",
    "    print (\"WITH year/month\")\n",
    "\n",
    "    parquet_path = os.path.join(\n",
    "        'computed', 'travels', f'travels.parquet')\n",
    "\n",
    "    df.write \\\n",
    "        .partitionBy(\"year\") \\\n",
    "        .mode(\"overwrite\").parquet(parquet_path)\n",
    "        # .write.option(\"maxRecordsPerFile\", 70000000) \\\n",
    "\n",
    "if ENABLE_COMPUTE_ZIP_TO_PARQUET :\n",
    "    process_csv_files(all_csv, schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Parquet is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(os.path.join(\"computed\", \"travels\", \"*.parquet\"))\n",
    "\n",
    "print(\"Number of rows : \" + str(df.count()))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension (Stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dimension = df.select(\"start_station_id\", \"start_station_name\", \n",
    "                                \"start_lat\", \"start_lng\") \\\n",
    "                        .withColumnRenamed(\"start_station_id\", \"station_id\") \\\n",
    "                        .withColumnRenamed(\"start_station_name\", \"station_name\") \\\n",
    "                        .withColumnRenamed(\"start_lat\", \"lat\") \\\n",
    "                        .withColumnRenamed(\"start_lng\", \"lng\")\n",
    "\n",
    "df_dimension = df_dimension.unionByName(\n",
    "    df.select(\"end_station_id\", \"end_station_name\", \n",
    "                    \"end_lat\", \"end_lng\") \\\n",
    "             .withColumnRenamed(\"end_station_id\", \"station_id\") \\\n",
    "             .withColumnRenamed(\"end_station_name\", \"station_name\") \\\n",
    "             .withColumnRenamed(\"end_lat\", \"lat\") \\\n",
    "             .withColumnRenamed(\"end_lng\", \"lng\")\n",
    "            ).distinct()\n",
    "\n",
    "                #, allowMissingColumns=True\n",
    "\n",
    "df_dimension = df_dimension.groupBy(\"station_id\").agg(\n",
    "    F.first(\"station_name\").alias(\"station_name\"),\n",
    "    F.first(\"lat\").alias(\"lat\"),\n",
    "    F.first(\"lng\").alias(\"lng\")\n",
    ")\n",
    "\n",
    "df_dimension.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_NOT_IGNORE_STAR_SCHEMA and ENABLE_COMPUTE_DIMENSION:\n",
    "    df_dimension.write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .parquet(os.path.join(\"computed\", \"dimension\", \"dimension.parquet\"))\n",
    "    #.option(\"maxRecordsPerFile\", 70000000) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_NOT_IGNORE_STAR_SCHEMA :\n",
    "    df_dimension = spark.read.parquet(os.path.join(\"computed\", \"dimension\", \"dimension.parquet\"))\n",
    "    # df_station_dim.show()\n",
    "    df_dimension.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_event_table(df_event):\n",
    "    columns = [\n",
    "        \"start_station_name\", \"start_lng\", \"start_lat\", \n",
    "        \"end_station_name\", \"end_lng\", \"end_lat\"\n",
    "        ]\n",
    "    \n",
    "    for x in columns :\n",
    "        df_event = df_event.drop(col(x))\n",
    "\n",
    "    return df_event\n",
    "\n",
    "df_event = make_event_table(df)\n",
    "df_event.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_NOT_IGNORE_STAR_SCHEMA and ENABLE_COMPUTE_EVENT :\n",
    "    df_event.write \\\n",
    "                .partitionBy(\"year\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .parquet(os.path.join(\"computed\", \"event\", \"event.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_NOT_IGNORE_STAR_SCHEMA :\n",
    "    df_event = spark.read.parquet(os.path.join(\"computed\", \"event\", \"event.parquet\"))\n",
    "    df_event.count()\n",
    "    df_event.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_star(df_event, df_dimension):\n",
    "    df_dimension_start = df_dimension.selectExpr(\n",
    "        \"station_id as start_station_id\",\n",
    "        \"station_name as start_station_name\",\n",
    "        \"lat as start_lat\",\n",
    "        \"lng as start_lng\"\n",
    "    )\n",
    "\n",
    "    df_dimension_end = df_dimension.selectExpr(\n",
    "        \"station_id as end_station_id\",\n",
    "        \"station_name as end_station_name\",\n",
    "        \"lat as end_lat\",\n",
    "        \"lng as end_lng\"\n",
    "    )\n",
    "\n",
    "    return df_event \\\n",
    "        .join(df_dimension_start, df_event.start_station_id == df_dimension_start.start_station_id, \"left\") \\\n",
    "        .join(df_dimension_end, df_event.end_station_id == df_dimension_end.end_station_id, \"left\")\n",
    "\n",
    "df_star = build_df_star(df_event, df_dimension)\n",
    "df_star.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('computed', 'graphics')) :\n",
    "    os.makedirs(os.path.join('computed', 'graphics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split before/after Covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date_pre_covid = '2020-02-29'\n",
    "start_date_post_covid = '2020-03-01'\n",
    "\n",
    "df_pre_covid = df.filter(col('ended_at') <= end_date_pre_covid)\n",
    "df_2018 = df.filter((col('ended_at') <= '2019-01-01') & (col('started_at') > '2018-01-01')) \n",
    "\n",
    "df_post_covid = df.filter(col('started_at') >= start_date_post_covid)\n",
    "df_2022 = df.filter((col('ended_at') <= '2023-01-01') & (col('started_at') > '2022-01-01')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Before Covid-19\")\n",
    "# df_pre_covid.show(5)\n",
    "# print(\"After Covid-19\")\n",
    "# df_post_covid.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute distances and durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Rayon de la Terre en kilomètres\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compte_trip_distance(df, verbose=False):\n",
    "    if verbose : print(\"compute distance\")\n",
    "\n",
    "    df = df.filter(\n",
    "        col(\"start_lat\").isNotNull() & \n",
    "        col(\"start_lng\").isNotNull() & \n",
    "        col(\"end_lat\").isNotNull() & \n",
    "        col(\"end_lng\").isNotNull()\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"distance_km\", haversine_udf(col(\"start_lat\"), \n",
    "                                     col(\"start_lng\"), \n",
    "                                     col(\"end_lat\"), \n",
    "                                     col(\"end_lng\")))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_2018 = compte_trip_distance(df_2018)\n",
    "df_2022 = compte_trip_distance(df_2022)\n",
    "df_post_covid = compte_trip_distance(df_post_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic function\n",
    "def visualize_trip_distance_over_something(metric, metric_name, df_1, df_2=None, description_1=\"\", description_2=\"\", min_factor=0.99):\n",
    "    def specific_chart(df, description_bonus):\n",
    "        min_value = df['avg_distance_km'].min() * min_factor\n",
    "        max_value = df['avg_distance_km'].max() * 1.02\n",
    "\n",
    "        return alt.Chart(df).mark_bar().encode(\n",
    "                x=alt.X(metric, title=metric_name, sort=None),\n",
    "                y=alt.Y('avg_distance_km:Q', title='Average distance (km)',\n",
    "                        scale=alt.Scale(domain=(min_value, max_value))),\n",
    "                color=alt.Color(metric, title=metric_name),\n",
    "                tooltip=['avg_distance_km', metric]\n",
    "            ).properties(\n",
    "                title= f'Average trip distance per {metric_name} {description_bonus}',\n",
    "                width= 300,\n",
    "                height= 200\n",
    "            )\n",
    "        \n",
    "\n",
    "    if df_2 is None:\n",
    "        chart = alt.hconcat(specific_chart(df_1, description_1))\n",
    "    else :\n",
    "        chart = alt.hconcat(\n",
    "            specific_chart(df_1, description_1),\n",
    "            specific_chart(df_2, description_2)\n",
    "        )\n",
    "\n",
    "    chart = chart.configure_axis(\n",
    "        labelAngle=45,\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10,\n",
    "        labelOverlap='parity'\n",
    "    ).configure_legend(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10\n",
    "    )\n",
    "\n",
    "    return chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graphic_data(graphic_data_generator, graphic_name, ENABLE_COMPUTE_GRAPHICS_DATA=ENABLE_COMPUTE_GRAPHICS_DATA) :\n",
    "    file_name = os.path.join(\"computed\", \"graphics\", f\"{graphic_name}.h5\")\n",
    "    if ENABLE_COMPUTE_GRAPHICS_DATA :\n",
    "        graphic_data = graphic_data_generator()\n",
    "        graphic_data.to_hdf(file_name, key='df', mode='w')\n",
    "        return graphic_data\n",
    "    return pd.read_hdf(file_name, key='df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip distance by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trip_distance_over_week(df, verbose=False):\n",
    "    '''\n",
    "        Returns in the pandas format\n",
    "    '''\n",
    "    if verbose : print(\"compute day of week\")\n",
    "    df = df.withColumn(\"day_of_the_week\", dayofweek(col(\"started_at\")))\n",
    "\n",
    "    if verbose : print(\"group by\")\n",
    "    \n",
    "    avg_distance_by_day_of_the_week = df.groupBy(\"day_of_the_week\") \\\n",
    "        .agg(avg(\"distance_km\").alias(\"avg_distance_km\"))\n",
    "    \n",
    "    avg_distance_by_day_of_the_week = avg_distance_by_day_of_the_week.withColumn(\n",
    "        \"day_of_the_week\",\n",
    "        when(col(\"day_of_the_week\") == 1, \"Sunday\")\n",
    "        .when(col(\"day_of_the_week\") == 2, \"Monday\")\n",
    "        .when(col(\"day_of_the_week\") == 3, \"Tuesday\")\n",
    "        .when(col(\"day_of_the_week\") == 4, \"Wednesday\")\n",
    "        .when(col(\"day_of_the_week\") == 5, \"Thursday\")\n",
    "        .when(col(\"day_of_the_week\") == 6, \"Friday\")\n",
    "        .when(col(\"day_of_the_week\") == 7, \"Saturday\")\n",
    "    )\n",
    "\n",
    "    return avg_distance_by_day_of_the_week.toPandas()\n",
    "\n",
    "trip_distance_over_week_2018 = save_graphic_data(lambda: compute_trip_distance_over_week(df_2018), \"trip_distance_over_week_2018\")\n",
    "trip_distance_over_week_2022 = save_graphic_data(lambda: compute_trip_distance_over_week(df_2022), \"trip_distance_over_week_2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trip_distance_over_something(\n",
    "    \"day_of_the_week\", \"day\",\n",
    "    trip_distance_over_week_2018, trip_distance_over_week_2022, \n",
    "    \" (2018)\", \" (2022)\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Number of trips for each pickup/dropoff location couple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_on_pickup_dropoff(df):\n",
    "    return df.groupBy(\n",
    "        \"start_station_id\", \"end_station_id\"\n",
    "        ).agg(\n",
    "            count(\"*\").alias(\"count\"),\n",
    "            F.concat_ws(\" -> \", \n",
    "                      F.first(\"start_station_name\"), \n",
    "                      F.first(\"end_station_name\")\n",
    "                      ).alias(\"station_pair\")\n",
    "        )\n",
    "    \n",
    "def reduce_pickup_dropoff(df):\n",
    "    return df.groupBy(\"count\") \\\n",
    "        .agg(count(\"*\").alias(\"frequency\")) \\\n",
    "        .toPandas()\n",
    "\n",
    "def top_10_pickup_dropoff(agg_df):\n",
    "    top_10_df = agg_df.orderBy(desc(\"count\")).limit(10)\n",
    "    return top_10_df.toPandas()\n",
    "\n",
    "def visualize_number_of_pickup_drop(ax, df, date):\n",
    "    sns.histplot(df[\"count\"], bins=30, kde=True, ax=ax)\n",
    "    ax.set_xlabel(\"Number of couple (start_station_id, end_station_id) during \" + date)\n",
    "    ax.set_ylabel(\"Number of occurrences the couple\")\n",
    "    ax.set_title(\"(start_station_id, end_station_id)\")\n",
    "\n",
    "def visualize_top_10_pickup_dropoff(ax, top_10_df, date):\n",
    "    sns.barplot(x=\"count\", y=\"station_pair\", data=top_10_df, dodge=False, ax=ax)\n",
    "    ax.set_xlabel(\"Number of Occurrences\")\n",
    "    ax.set_ylabel(\"Station Pair\")\n",
    "    ax.set_title(\"Top 10 Most Frequent Start-End Station Pairs during \" + date)\n",
    "    \n",
    "    labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "    wrapped_labels = [\"\\n\".join(label.split(\" -> \")) for label in labels]\n",
    "    ax.set_yticklabels(wrapped_labels)\n",
    "\n",
    "def full_visualization(df, date):\n",
    "    agg = aggregate_on_pickup_dropoff(df)\n",
    "\n",
    "    df_reduce_pickup_dropoff = save_graphic_data(lambda: reduce_pickup_dropoff(agg), f\"reduce_pickup_dropoff_{date}\")\n",
    "    df_top_10_pickup_dropoff = save_graphic_data(lambda: top_10_pickup_dropoff(agg), f\"top_10_pickup_dropoff_{date}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    visualize_number_of_pickup_drop(axes[0], df_reduce_pickup_dropoff, date)\n",
    "    visualize_top_10_pickup_dropoff(axes[1], df_top_10_pickup_dropoff, date)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_visualization(df_2018, \"2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_visualization(df_2022, \"2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip distance distribution for gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trip_distance_over_genders(df, verbose=False):\n",
    "    '''\n",
    "        Returns in Pandas format\n",
    "    '''\n",
    "    df = df.filter(col(\"gender\").isNotNull())\n",
    "    \n",
    "    avg_distance = df.groupBy(\"gender\") \\\n",
    "        .agg(avg(\"distance_km\").alias(\"avg_distance_km\"))\n",
    "    \n",
    "    return avg_distance.toPandas()\n",
    "\n",
    "trip_distance_over_genders_2018 = save_graphic_data(lambda: compute_trip_distance_over_genders(df_2018), \"trip_distance_over_genders_2018\")\n",
    "trip_distance_over_genders_post_covid = save_graphic_data(lambda: compute_trip_distance_over_genders(df_post_covid), \"trip_distance_over_genders_post_covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trip_distance_over_something(\n",
    "    \"gender\", \"Gender\",\n",
    "    trip_distance_over_genders_2018, trip_distance_over_genders_post_covid,\n",
    "    \" (2018)\", \" (post covid)\", min_factor=0.0\n",
    "    ).interactive()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip distance distribution for age ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trip_distance_over_ages(df, verbose=False):\n",
    "    '''\n",
    "        Returns in Pandas format\n",
    "    '''\n",
    "        \n",
    "    df = df.filter(col(\"birth_year\").isNotNull())\n",
    "    df = df.withColumn(\"age\", year(col('started_at')) - col('birth_year'))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"age_range\", \n",
    "         when(col('age') <= 14, lit(None) ) \\\n",
    "        .when((15 <= col('age')) & (col('age') <= 24), lit('15-24')) \\\n",
    "        .when((25 <= col('age')) & (col('age') <= 44), lit('25-45')) \\\n",
    "        .when((45 <= col('age')) & (col('age') <= 54), lit('45-54')) \\\n",
    "        .when((55 <= col('age')) & (col('age') <= 64), lit('55-64')) \\\n",
    "        .when(65 <= col('age'), lit('65+')) \\\n",
    "    )\n",
    "\n",
    "    avg_distance = df.groupBy(\"age_range\") \\\n",
    "        .agg(avg(\"distance_km\").alias(\"avg_distance_km\"))\n",
    "\n",
    "    return avg_distance.toPandas()\n",
    "\n",
    "trip_distance_over_ages_2018 = save_graphic_data(lambda: compute_trip_distance_over_ages(df_2018), \"trip_distance_over_ages_2018\")\n",
    "trip_distance_over_ages_post_covid = save_graphic_data(lambda: compute_trip_distance_over_ages(df_post_covid), \"trip_distance_over_ages_post_covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trip_distance_over_something(\n",
    "    \"age_range\", \"Age Range\",\n",
    "    trip_distance_over_ages_2018, trip_distance_over_ages_post_covid,\n",
    "    \" (2018) \", \" (post covid) \", min_factor=0.0\n",
    "    ).interactive()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip distance distribution for different kind of bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trip_distance_over_bikes(df, verbose=False):\n",
    "    df = df.filter(col(\"rideable_type\").isNotNull())\n",
    "    \n",
    "    avg_distance = df.groupBy(\"rideable_type\") \\\n",
    "        .agg(avg(\"distance_km\").alias(\"avg_distance_km\"))\n",
    "\n",
    "    return avg_distance.toPandas()\n",
    "\n",
    "trip_distance_over_bikes_post_covid = save_graphic_data(lambda: compute_trip_distance_over_bikes(df_post_covid), \"trip_distance_over_bikes_post_covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trip_distance_over_something(\n",
    "    \"rideable_type\", \"Type of bike\",\n",
    "    trip_distance_over_bikes_post_covid, None,\n",
    "    \" (after covid) \", min_factor=0.0\n",
    "    ).interactive()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hour of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hour_of_the_week(df):\n",
    "    df = df.dropna(subset=[\"started_at\"])\n",
    "    df = df.withColumn(\"day_of_the_week\", dayofweek(col(\"started_at\")) - 1)\n",
    "    df = df.withColumn(\"hour_of_the_day\", hour(col(\"started_at\")))\n",
    "    df = df.withColumn(\"hour_of_the_week\", \n",
    "                col(\"day_of_the_week\")*24 + col(\"hour_of_the_day\"))\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"day_of_the_week\",\n",
    "        when(col(\"day_of_the_week\") == 0, \"Sunday\")\n",
    "        .when(col(\"day_of_the_week\") == 1, \"Monday\")\n",
    "        .when(col(\"day_of_the_week\") == 2, \"Tuesday\")\n",
    "        .when(col(\"day_of_the_week\") == 3, \"Wednesday\")\n",
    "        .when(col(\"day_of_the_week\") == 4, \"Thursday\")\n",
    "        .when(col(\"day_of_the_week\") == 5, \"Friday\")\n",
    "        .when(col(\"day_of_the_week\") == 6, \"Saturday\")\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_data_per_hour_of_the_week(df):\n",
    "    result = df.groupBy(\"hour_of_the_week\", \"day_of_the_week\", \"hour_of_the_day\") \\\n",
    "    .agg(\n",
    "        F.countDistinct(\n",
    "            \"start_station_id\", \"end_station_id\"\n",
    "            ).alias(\"distinct_station_pairs\"),\n",
    "        avg(\"distance_km\").alias(\"avg_distance_km\"),\n",
    "        avg(\"trip_duration\").alias(\"avg_trip_duration\"),\n",
    "        count(\"*\").alias(\"row_count\")\n",
    "    )\n",
    "\n",
    "    return result.toPandas().sort_values(\"hour_of_the_week\").reset_index(drop=True)\n",
    "\n",
    "f = lambda x: compute_data_per_hour_of_the_week(compute_hour_of_the_week(x))\n",
    "\n",
    "data_phw_2018 = save_graphic_data(lambda: f(df_2018), \"data_phw_2018\")\n",
    "data_phw_2022 = save_graphic_data(lambda: f(df_2022), \"data_phw_2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2018\");data_phw_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2022\");data_phw_2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_visualization_over_the_week(metric, metric_name, df_1, df_2, description_1=\"\", description_2=\"\"):\n",
    "    def specific_chart(df, description_bonus):\n",
    "        min_value = 0 # df['avg_distance_km'].min() * 0.80\n",
    "        max_value = df['avg_distance_km'].max() * 1.02\n",
    "\n",
    "        return alt.Chart(df).mark_bar().encode(\n",
    "                x=alt.X('hour_of_the_week:N', title='Hour of the week', sort=None),\n",
    "                y=alt.Y(metric, title=metric_name), #scale=alt.Scale(domain=(min_value, max_value))),\n",
    "                color=alt.Color('day_of_the_week', title='Day of the week'),\n",
    "                tooltip=[metric, 'day_of_the_week', 'hour_of_the_day']\n",
    "            ).properties(\n",
    "                title= f'{metric_name} per hour of the week ' + description_bonus,\n",
    "                width= 300,\n",
    "                height= 200\n",
    "            )\n",
    "        \n",
    "    chart = alt.hconcat(\n",
    "        specific_chart(df_1, description_1),\n",
    "        specific_chart(df_2, description_2)\n",
    "    ).configure_axis(\n",
    "        labelAngle=45\n",
    "    )\n",
    "\n",
    "    chart = chart.configure_axis(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10,\n",
    "        labelOverlap='parity'\n",
    "    ).configure_legend(\n",
    "        titleFontSize=12,\n",
    "        labelFontSize=10\n",
    "    )\n",
    "\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_visualization_over_the_week_1822 = lambda x, y : full_visualization_over_the_week(x, y, data_phw_2018, data_phw_2022, \"(2018)\", \"(2022)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of pickups/docks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_visualization_over_the_week_1822(\"distinct_station_pairs\", \"Distinct Station Pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_visualization_over_the_week_1822(\"avg_distance_km\", \"Average distance (km)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average trip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_visualization_over_the_week_1822(\"avg_trip_duration\", \"Average trip duration (seconds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average number of ongoing trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_visualization_over_the_week_1822(\"row_count\", \"Number of ongoing trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Job to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_analyze = spark.read.parquet(os.path.join(\"computed\", \"travels\", \"*.parquet\"))\n",
    "# df_to_analyze.show()\n",
    "\n",
    "# df_star.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also analyze some toPandas performed for visualization graphics above in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Travels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Relation does not differ between Analyzed and Optimized Logical Plan. The query optimizer in the RDBMS did not find any opportunities to further optimize the logical plan after the initial analysis phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The Physical Plan differs from the Optimized Logical Plan in that it includes the ColumnarToRow operator, which is a Spark-specific operator that converts data from the columnar format used for storage and processing to the row format used for execution. The FileScan parquet operator is also specific to Spark and is used to scan Parquet data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "    - It requires :\n",
    "        - One stage to read the parquet file of `travels`.\n",
    "        - One stage to show it.\n",
    "        - Two stages (over two jobs) to show the df_star \n",
    "    - **HashAggregate** efficiently performs aggregations by grouping data using hash tables, optimizing memory usage and processing speed. \n",
    "    - **Exchange hashpartitioning** redistributes data across nodes based on hashed keys, enabling parallel processing and necessary data colocation for operations like joins, albeit with the cost of shuffle operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The above operations do not perform shuffle operations. But, neverthless, if we look down on jobs performing at .toPandas operation, they do perform a shuffle operation to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. \n",
    "    - Tasks are the smallest units of work within a stage, each processing a single data partition.\n",
    "    - Again, for the 3 operations shown in this part, there is one task (except for df_star which uses 2 stages in one of its tasks). Nevertheless, most toPandas stages perform **8** task (because, the `travels.parquet` is partionned into the **8**`** years present in the dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Informations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_heatmap_data(df, fraction):\n",
    "    trip_counts = df.groupBy(\"start_station_id\", \"end_station_id\").count()\n",
    "    trip_counts = trip_counts.withColumnRenamed(\"count\", \"trip_count\")\n",
    "    if fraction != 1.0 :\n",
    "        trip_counts = trip_counts.sample(withReplacement=False, fraction=fraction)\n",
    "    trip_counts = trip_counts.toPandas()\n",
    "    return trip_counts.sort_values(\"trip_count\") # sort by count ?\n",
    "\n",
    "def preview_heat_map(heatmap_data):\n",
    "    heatmap_data = heatmap_data.pivot(index='start_station_id', columns='end_station_id', values='trip_count')\n",
    "    return px.imshow(heatmap_data, title=\"Heatmap of Start/End stations in 2018\", labels=dict(x=\"End Station\", y=\"Start Station\", color=\"Number of Trips\"))\n",
    "\n",
    "FRACTION = 1.0 # my computer is not powerful enough\n",
    "heatmap_data = save_graphic_data(lambda: build_heatmap_data(df_2018, FRACTION), \"heatmap_data_2018\")\n",
    "preview_heat_map(heatmap_data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spatial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interactive_map_data(df):\n",
    "    start_df = df.select(col(\"start_station_id\").alias(\"station_id\"),\n",
    "                        hour(col(\"started_at\")).alias(\"hour\"),\n",
    "                        col(\"start_lat\").alias(\"lat\"),\n",
    "                        col(\"start_lng\").alias(\"lng\"))\n",
    "\n",
    "    end_df = df.select(col(\"end_station_id\").alias(\"station_id\"),\n",
    "                        hour(col(\"ended_at\")).alias(\"hour\"),\n",
    "                        col(\"end_lat\").alias(\"lat\"),\n",
    "                        col(\"end_lng\").alias(\"lng\"))\n",
    "\n",
    "    union_df = start_df.union(end_df)\n",
    "\n",
    "    hourly_trip_counts = union_df.groupBy(\"hour\", \"station_id\", \"lat\", \"lng\").count()\n",
    "\n",
    "    return hourly_trip_counts.toPandas()\n",
    "\n",
    "interactive_map_data = save_graphic_data(lambda: build_interactive_map_data(df_2018), \"interactive_map_data_2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_heatmap(df, bonus=\"\"):\n",
    "    df = df.sort_values(\"hour\")\n",
    "    fig = px.density_mapbox(df, lat='lat', lon='lng', z='count', radius=30,\n",
    "                            center=dict(lat=40.7128, lon=-74.0060), zoom=10,\n",
    "                            mapbox_style=\"carto-positron\", animation_frame='hour',\n",
    "                            range_color=[0, df['count'].max()*1.0])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Citibikes Station Frequency Heatmap of New York City by Hour\" + bonus,\n",
    "        updatemenus=[dict(\n",
    "            type=\"buttons\",\n",
    "            showactive=False,\n",
    "            buttons=[\n",
    "                dict(label=\"Play\",\n",
    "                     method=\"animate\",\n",
    "                     args=[None, dict(frame=dict(duration=500, redraw=True), fromcurrent=True)]),\n",
    "                dict(label=\"Pause\",\n",
    "                     method=\"animate\",\n",
    "                     args=[[None], dict(frame=dict(duration=0, redraw=False), mode=\"immediate\", fromcurrent=True)])\n",
    "            ]\n",
    "        )],\n",
    "        sliders = [dict(\n",
    "            active=0,\n",
    "            currentvalue={\"prefix\": \"Hour: \"},\n",
    "            pad={\"t\": 1}\n",
    "        )],\n",
    "        height=700, width=700 # margin=dict(l=0, r=0, t=10.0, b=0)\n",
    "        \n",
    "    )\n",
    "\n",
    "    fig.show(frame=0)\n",
    "\n",
    "display_heatmap(interactive_map_data, \" (2018)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
