{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Project II - Big Data\n",
    "\n",
    "Author : Ophiase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_DOWNLOAD = False\n",
    "ENABLE_UNZIP = False\n",
    "ENABLE_COMPUTE_ZIP_TO_PARQUET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyspark.sql.functions as F\n",
    "import zipfile\n",
    "from functools import reduce\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--driver-memory 2g\"\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import when, mean, stddev, skewness, kurtosis, expr, date_format\n",
    "import pyspark.sql.functions as pf\n",
    "from pyspark.sql.functions import col, lit, udf, dayofweek, avg\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/20 17:51:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Big Data Project\") \\\n",
    "    .config(\"spark.driver.memory\", \"3g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# from 2014 to 2023\n",
    "trip_urls = [\n",
    "    (year, f\"https://s3.amazonaws.com/tripdata/{year}-citibike-tripdata.zip\")\n",
    "    for year in range(2014, 2023 + 1)\n",
    "]\n",
    "\n",
    "if not os.path.exists(os.path.join('data')) :\n",
    "    os.makedirs('data')\n",
    "\n",
    "zip_files = []\n",
    "\n",
    "for year, url in trip_urls:\n",
    "    basename = os.path.join('data', str(year) + \"_\" + 'citibike_tripdata')\n",
    "    zip_filename = basename + \".zip\"\n",
    "    csv_filename = basename + \".csv\"\n",
    "    zip_files.append((year, zip_filename, csv_filename))\n",
    "\n",
    "    if not ENABLE_DOWNLOAD : continue\n",
    "    print(f'Check {basename} ...')\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "    progress = 0\n",
    "\n",
    "    if not os.path.exists(zip_filename) and not os.path.exists(csv_filename) :\n",
    "        with open(zip_filename, 'wb') as f:\n",
    "            for data in response.iter_content(block_size):\n",
    "                if data:\n",
    "                    f.write(data)\n",
    "                    progress += len(data)\n",
    "                    print(f'\\rDownloaded {progress}/{total_size} bytes', end='')\n",
    "\n",
    "        print(f'\\nDownload complete: {zip_filename}')\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_UNZIP :\n",
    "    for (year, zip_filename, csv_filename) in zip_files:\n",
    "        # if year < 2018: continue # WARNING : DISABLE THIS LINE\n",
    "\n",
    "        if not zipfile.is_zipfile(zip_filename):\n",
    "            print(\"Corrupted zip file.\")\n",
    "            break\n",
    "\n",
    "        if os.path.exists(\"tmp\"):\n",
    "            shutil.rmtree(\"tmp\")\n",
    "\n",
    "        print(\"Unzip : \", zip_filename)\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"tmp\")\n",
    "        print(\"Process ..\")\n",
    "\n",
    "        # find the folder in tmp\n",
    "        items = os.listdir(\"tmp\")\n",
    "        for folder in items :\n",
    "            if not os.path.isdir(os.path.join(\"tmp\", folder)) or \\\n",
    "                folder.startswith(\"__\") :\n",
    "                continue\n",
    "            \n",
    "            # find all the folder in this folder\n",
    "            sub_folders = os.listdir(os.path.join(\"tmp\", folder))\n",
    "            for sub_folder in sub_folders :\n",
    "                if not os.path.isdir(os.path.join(\"tmp\", folder, sub_folder)) or \\\n",
    "                    sub_folder.startswith(\".\") : \n",
    "                    continue\n",
    "                \n",
    "                sub_item = os.listdir(os.path.join(\"tmp\", folder, sub_folder))\n",
    "                for leaf in sub_item :\n",
    "                    # move the csv inside to data\n",
    "                    from_path = os.path.join(\"tmp\", folder, sub_folder, leaf)\n",
    "                    dest_path = os.path.join(\"data\", leaf)\n",
    "                    if os.path.exists(dest_path) : \n",
    "                        os.remove(dest_path)\n",
    "\n",
    "                    shutil.move(from_path, \"data\")\n",
    "\n",
    "    if os.path.exists(\"tmp\"):\n",
    "        shutil.rmtree(\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize disk usage, we could have unziped one file at a time and convert its content instantaneously to `.parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader = spark.read.option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\").csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_csv():\n",
    "    all_csv = []\n",
    "    for item in os.listdir(\"data\"):\n",
    "        if not item.endswith(\".csv\") :\n",
    "            continue\n",
    "        all_csv.append(item)\n",
    "    return sorted(all_csv)\n",
    "\n",
    "all_csv = find_all_csv()\n",
    "\n",
    "if False: # check column_names.txt\n",
    "    for item in all_csv:    \n",
    "        df = csv_reader(os.path.join(\"data\", item))\n",
    "        print(f\"item {item} : {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the previous code output *(cached in `column_names.txt`)*, \\\n",
    "we notice the following columns between 2014-01 $\\to$ 2021-01 (included) :\n",
    "- `['tripduration', 'starttime', 'stoptime', 'start station id', 'start station name', `\\\n",
    "`'start station latitude', 'start station longitude', 'end station id', 'end station name', `\\\n",
    "`'end station latitude', 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender']`\n",
    "    - The naming convention is not exactly the same between : `201610-citibike-tripdata_1.csv` $\\to$ `201703-citibike-tripdata.csv_1.csv` : \\\n",
    "    `['Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID',` \\\n",
    "    `'Start Station Name', 'Start Station Latitude', 'Start Station Longitude',` \\\n",
    "    `'End Station ID', 'End Station Name', 'End Station Latitude',` \\\n",
    "    `'End Station Longitude', 'Bike ID', 'User Type', 'Birth Year', 'Gender']`\n",
    "\n",
    "The columns change between 2021-02 $\\to$ 2023-12 (included) :\n",
    "- `['ride_id', 'rideable_type', 'started_at', 'ended_at', 'start_station_name', `\\\n",
    "`'start_station_id', 'end_station_name', 'end_station_id', 'start_lat', `\\\n",
    "`'start_lng', 'end_lat', 'end_lng', 'member_casual']`\n",
    "\n",
    "We decide the following matching (**O.** as before 2021-02, **N.** as after 2021-02):\n",
    "\n",
    "- `O.'tripduration'` as function : `stoptime` - `startime`\n",
    "    - Renamed as `trip_duration`\n",
    "\n",
    "- `N.'started_at'` $\\leftarrow$ `O.'starttime'`\n",
    "- `N.'ended_at'` $\\leftarrow$ `O.'stoptime'` \n",
    "- `N.'start_station_id'` $\\leftarrow$ `O.'start station id'`\n",
    "    - type : string\n",
    "- `N.'start_station_name'` $\\leftarrow$ `O.'start station name'`\n",
    "- `N.'start_lat'` $\\leftarrow$ `O.'start station latitude'`\n",
    "- `N.'start_lng'` $\\leftarrow$ `O.'start station longitude'`\n",
    "- `N.'end_station_id'` $\\leftarrow$ `O.'end station id'`\n",
    "    - type : string\n",
    "- `N.'end_station_name'` $\\leftarrow$ `O.'end station name'`\n",
    "- `N.'end_lat'` $\\leftarrow$ `O.'end station latitude'`\n",
    "- `N.'end_lng'` $\\leftarrow$ `O.'end station longitude'`\n",
    "`\n",
    "- `N.'ride_id'` $\\leftarrow$ `O.'bikeid'` (format is not the same)\n",
    "    - Both type of ID can registered as string\n",
    "\n",
    "- `N.'member_casual'` $\\leftarrow$ `O.'usertype'` (format is not the same)\n",
    "    - Mapping : `O.Subscriber`, `O.Customer` $\\to$ `N.member`, `N.casual`\n",
    "\n",
    "- `O.'birth year` : (None) for elements of N\n",
    "    - Renamed as `birth_year`\n",
    "- `O.'gender'` : (None) for elements of N\n",
    "- `N.'rideable_type'` : (None) for elements of O\n",
    "\n",
    "- We will also add a binary column `old_format` to indicate if the data comes from `O` or `N` as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mapping_1 = {\n",
    "    'tripduration': 'trip_duration',\n",
    "    'usertype': 'member_casual',\n",
    "    'birth year': 'birth_year',\n",
    "\n",
    "    'starttime': 'started_at',\n",
    "    'stoptime': 'ended_at',\n",
    "    'start station id': 'start_station_id',\n",
    "    'start station name': 'start_station_name',\n",
    "    'start station latitude': 'start_lat',\n",
    "    'start station longitude': 'start_lng',\n",
    "    'end station id': 'end_station_id',\n",
    "    'end station name': 'end_station_name',\n",
    "    'end station latitude': 'end_lat',\n",
    "    'end station longitude': 'end_lng',\n",
    "    'bikeid': 'ride_id',\n",
    "}\n",
    "\n",
    "col_mapping_2 = {\n",
    "    'Trip Duration': 'tripduration',\n",
    "    'Start Time': 'starttime',\n",
    "    'Stop Time': 'stoptime',\n",
    "    \n",
    "    'Start Station ID': 'start station id',\n",
    "    'Start Station Name': 'start station name',\n",
    "    'Start Station Latitude': 'start station latitude',\n",
    "    'Start Station Longitude': 'start station longitude',\n",
    "\n",
    "    'End Station ID': 'end station id',\n",
    "    'End Station Name' : 'end station name',\n",
    "    'End Station Latitude' : 'end station latitude',\n",
    "    'End Station Longitude' : 'end station longitude',\n",
    "    \n",
    "    'Bike ID' : 'bikeid',\n",
    "    'User Type' : 'usertype',\n",
    "    'Birth Year' : 'birth year',\n",
    "    'Gender' : 'gender'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df, column):\n",
    "    return df.select(column).dropDuplicates().rdd.map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subscriber', 'Customer']\n",
      "[1, 2, 0]\n",
      "['electric_bike', 'classic_bike']\n",
      "root\n",
      " |-- trip_duration: integer (nullable = true)\n",
      " |-- started_at: timestamp (nullable = true)\n",
      " |-- ended_at: timestamp (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_lat: double (nullable = true)\n",
      " |-- start_lng: double (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_lat: double (nullable = true)\n",
      " |-- end_lng: double (nullable = true)\n",
      " |-- ride_id: integer (nullable = true)\n",
      " |-- member_casual: string (nullable = true)\n",
      " |-- birth_year: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ride_id: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- started_at: timestamp (nullable = true)\n",
      " |-- ended_at: timestamp (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- start_lat: double (nullable = true)\n",
      " |-- start_lng: double (nullable = true)\n",
      " |-- end_lat: double (nullable = true)\n",
      " |-- end_lng: double (nullable = true)\n",
      " |-- member_casual: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fast_check():\n",
    "    df_o = spark.read.csv(os.path.join(\"data\", all_csv[0]), header=True, inferSchema=True)\n",
    "    df_o = df_o.select(\n",
    "        [col(old_col).alias(col_mapping_1.get(old_col, old_col)) for old_col in df_o.columns]\n",
    "        )\n",
    "\n",
    "    df_n = spark.read.csv(os.path.join(\"data\", all_csv[-1]), header=True, inferSchema=True)\n",
    "    \n",
    "    print(check_unique_values(df_o, \"member_casual\"))\n",
    "    print(check_unique_values(df_o, \"gender\"))\n",
    "    print(check_unique_values(df_n, \"rideable_type\"))\n",
    "\n",
    "    df_o.printSchema()\n",
    "    df_n.printSchema()\n",
    "    \n",
    "    # df.show()\n",
    "\n",
    "fast_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File format Selection : Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/file-formats.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Our [course](https://stephane-v-boucheron.fr/slides/tbd/slides06_file-formats.html#/title-slide) on Big Data file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet suits our needs for the project:\n",
    "\n",
    "- Suitable for laptop execution\n",
    "- 37GB dataset size manageable with Parquet's compression\n",
    "- Supports splitability for processing subsets of data on minimal configuration\n",
    "- Compatible with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV $\\to$ Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('computed')) :\n",
    "    os.makedirs('computed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial_columns = [\"member_casual\", \"gender\", \"rideable_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Case O. part\n",
    "\n",
    "# add missing columns\n",
    "def csv_o_process(df):\n",
    "    df = df.select(\n",
    "        [col(old_col).alias(col_mapping_1.get(old_col, old_col)) for old_col in df.columns])\n",
    "\n",
    "    df = df.withColumn(\"birth_year\", \n",
    "                       when(col(\"birth_year\") == r\"\\N\", None)\n",
    "                       .otherwise(col(\"birth_year\"))\n",
    "                       .cast(\"integer\")\n",
    "                       )\n",
    "\n",
    "    df = df.withColumn(\"start_station_id\", col(\"start_station_id\").cast(\"string\"))\\\n",
    "            .withColumn(\"end_station_id\", col(\"end_station_id\").cast(\"string\")) \\\n",
    "            .withColumn(\"ended_at\", col(\"ended_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"started_at\", col(\"started_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"ride_id\", col(\"ride_id\").cast(\"string\"))\n",
    "\n",
    "    df = df.withColumn(\"member_casual\",\n",
    "                        when(col(\"member_casual\") == \"Subscriber\", lit(\"member\")) \\\n",
    "                        .otherwise(lit(\"casual\")))\n",
    "    \n",
    "    df = df.withColumn(\"rideable_type\", lit(None).cast('string'))\n",
    "    df = df.withColumn(\"old_format\", lit(True))\n",
    "    \n",
    "    df = df.select(*sorted(df.columns))\n",
    "\n",
    "    # df.write.mode(\"append\").parquet(parquet_file)\n",
    "    return df\n",
    "\n",
    "def csv_to_parquet_part_1(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    return csv_o_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper case O. part\n",
    "\n",
    "def csv_to_parquet_part_2(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    \n",
    "    df = df.select(\n",
    "        [col(old_col).alias(col_mapping_2.get(old_col, old_col)) for old_col in df.columns])\n",
    "    \n",
    "    return csv_o_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N. Part\n",
    "\n",
    "def csv_to_parquet_part_3(csv_file) :\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "\n",
    "    df = df.withColumn(\"trip_duration\", lit(None).cast(\"integer\")) # TODO\n",
    "    df = df.withColumn(\"old_format\", lit(False))\n",
    "    df = df.withColumn(\"birth_year\", lit(None).cast(\"integer\"))\n",
    "    df = df.withColumn(\"gender\", lit(0)) # TODO verify the default value\n",
    "    df = df.withColumn(\"old_format\", lit(False))\n",
    "\n",
    "    df = df.withColumn(\"start_station_id\", col(\"start_station_id\").cast(\"string\"))\\\n",
    "            .withColumn(\"end_station_id\", col(\"end_station_id\").cast(\"string\")) \\\n",
    "            .withColumn(\"ended_at\", col(\"ended_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"started_at\", col(\"started_at\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"ride_id\", col(\"ride_id\").cast(\"string\"))\n",
    "\n",
    "    df = df.select(*sorted(df.columns))\n",
    "\n",
    "    # df.write.mode(\"append\").parquet(parquet_file)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler\n",
    "\n",
    "schedule = [\n",
    "    (\"201401-citibike-tripdata_1.csv\", 1),\n",
    "    (\"201610-citibike-tripdata_1.csv\", 2),\n",
    "    (\"201704-citibike-tripdata.csv_1.csv\", 1),\n",
    "    (\"202102-citibike-tripdata_1.csv\", 3)\n",
    "]\n",
    "\n",
    "# The file size are always below 180M\n",
    "# We can safely load simultaneously 20 files from memory to .parquet\n",
    "CSV_PER_PARQUET = 10\n",
    "\n",
    "def reduce_df_array(df_array):\n",
    "    # if not df_array:\n",
    "    #     raise ValueError(\"Empty array.\")\n",
    "\n",
    "    # schema = df_array[0].schema\n",
    "    # for df in df_array:\n",
    "    #     if df.schema != schema:\n",
    "    #         raise ValueError(\"Incompatibles schema.\")\n",
    "\n",
    "    return reduce(lambda df1, df2: df1.union(df2), df_array)\n",
    "\n",
    "def process_csv_files(csv_files, schedule, parquet_dir=\"computed\", max_size_gb=1, start_with=None):\n",
    "    schedule_pointer = 0\n",
    "    parquet_index = 0\n",
    "\n",
    "    df_buffer = []\n",
    "\n",
    "    # ignore = start_with is not None\n",
    "    for index, csv_file in enumerate(csv_files):\n",
    "        # if ignore:\n",
    "        #     if csv_file != start_with : continue\n",
    "        #     ignore = False\n",
    "        #     parquet_index = index // CSV_PER_PARQUET\n",
    "\n",
    "        csv_path = os.path.join(\"data\", csv_file)\n",
    "        \n",
    "        print(f\"[] {csv_file}\")\n",
    "        if (schedule_pointer < len(schedule) - 1) and (csv_file == schedule[schedule_pointer + 1][0]) :\n",
    "            schedule_pointer += 1\n",
    "            print(f\"{csv_file} | {schedule_pointer} : {schedule[schedule_pointer]}\")\n",
    "\n",
    "        schedule_mode = schedule[schedule_pointer][1]\n",
    "        target_function = [csv_to_parquet_part_1, csv_to_parquet_part_2, csv_to_parquet_part_3][schedule_mode - 1]\n",
    "\n",
    "        df_buffer.append(target_function(csv_path))\n",
    "\n",
    "        if (index + 1) % CSV_PER_PARQUET == 0 :\n",
    "            parquet_path = os.path.join(\n",
    "                parquet_dir, 'precompiled', f'part_{parquet_index:04d}.parquet')\n",
    "            print(f\"Writing ({index}/{len(csv_files)}) : {parquet_dir}\")\n",
    "            print(f\"From {csv_files[index-CSV_PER_PARQUET+1]} to {csv_files[index]}\")\n",
    "\n",
    "            df = reduce_df_array(df_buffer)\n",
    "            df_buffer = []\n",
    "            parquet_index += 1\n",
    "\n",
    "            df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "if ENABLE_COMPUTE_ZIP_TO_PARQUET:\n",
    "    process_csv_files(all_csv, schedule) #, start_with=\"202011-citibike-tripdata_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Parquet is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195252960\n",
      "+----------+-----------------+------------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+\n",
      "|birth_year|          end_lat|           end_lng|end_station_id|    end_station_name|           ended_at|gender|member_casual|old_format|         ride_id|rideable_type|        start_lat|         start_lng|start_station_id|  start_station_name|         started_at|trip_duration|\n",
      "+----------+-----------------+------------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+\n",
      "|      NULL|40.72970805644994| -73.9865979552269|       5746.02|     E 10 St & 2 Ave|2023-09-03 10:24:16|     0|       member|     false|B0A0F1DEFA4B72FC|electric_bike|40.72486122254819|-73.99213135242462|         5636.13|     E 1 St & Bowery|2023-09-03 10:20:41|         NULL|\n",
      "|      NULL|      40.72019576|      -73.98997825|       5414.06|Allen St & Riving...|2023-09-27 15:53:25|     0|       member|     false|2B26AB15647BF4EE| classic_bike|     40.704650044|     -74.009132624|         4993.02|Pearl St & Hanove...|2023-09-27 15:44:23|         NULL|\n",
      "|      NULL|40.72970805644994| -73.9865979552269|       5746.02|     E 10 St & 2 Ave|2023-09-19 13:48:11|     0|       member|     false|9D2B5971CA4E513F| classic_bike|     40.724753261|     -73.992115974|         5636.13|     E 1 St & Bowery|2023-09-19 13:40:48|         NULL|\n",
      "|      NULL|      40.72317958|      -73.99480012|       5561.04| Mott St & Prince St|2023-09-30 16:56:35|     0|       member|     false|17E6760596DC3ABE| classic_bike|     40.696706414|     -73.922934651|         4713.01|Central Ave & Him...|2023-09-30 16:27:50|         NULL|\n",
      "|      NULL|      40.72317958|      -73.99480012|       5561.04| Mott St & Prince St|2023-09-21 17:07:36|     0|       member|     false|97EFF376A7E2DC70| classic_bike|       40.7284186|      -73.98713956|         5669.10| St Marks Pl & 2 Ave|2023-09-21 16:59:53|         NULL|\n",
      "|      NULL|         40.73564|         -73.95866|       5944.01|Franklin St & Dup...|2023-09-11 14:54:47|     0|       member|     false|D14BD5627029FFEA| classic_bike|     40.726941943|     -73.953004837|         5666.04|Meserole Ave & Ma...|2023-09-11 14:50:19|         NULL|\n",
      "|      NULL|40.70960089363537|-74.00655090808868|       5137.11|Fulton St & Willi...|2023-09-06 17:23:40|     0|       member|     false|D4AF7211BE4B6C54| classic_bike|       40.7047177|      -74.00926027|         4993.02|Pearl St & Hanove...|2023-09-06 17:19:49|         NULL|\n",
      "|      NULL|      40.68650065|      -73.96563307|       4419.03|Washington Ave & ...|2023-09-26 18:04:14|     0|       member|     false|775E8EF8E42CF867| classic_bike|     40.701164842|     -73.988743901|         4821.10|Adams St & Prospe...|2023-09-26 17:48:10|         NULL|\n",
      "|      NULL|      40.72019576|      -73.98997825|       5414.06|Allen St & Riving...|2023-09-15 14:18:07|     0|       casual|     false|2CC42BB00C45A968| classic_bike|         40.72323|         -74.00314|         5569.07|West Broadway & W...|2023-09-15 13:14:38|         NULL|\n",
      "|      NULL|        40.824796|         -73.90242|       7991.01|Tinton Ave & E 16...|2023-09-06 03:41:42|     0|       member|     false|7FEB0C3990129D56| classic_bike|     40.796563148|     -73.934445858|         7579.11|    E 117 St & 1 Ave|2023-09-06 03:29:02|         NULL|\n",
      "|      NULL|40.70960089363537|-74.00655090808868|       5137.11|Fulton St & Willi...|2023-09-13 08:30:54|     0|       casual|     false|B54C7B22E82BFD7A|electric_bike|       40.7047177|      -74.00926027|         4993.02|Pearl St & Hanove...|2023-09-13 08:27:35|         NULL|\n",
      "|      NULL|      40.72317958|      -73.99480012|       5561.04| Mott St & Prince St|2023-09-13 07:08:40|     0|       casual|     false|1BA80F46022B4613| classic_bike|      40.72852397|     -73.987239361|         5669.10| St Marks Pl & 2 Ave|2023-09-13 07:03:45|         NULL|\n",
      "|      NULL|      40.72317958|      -73.99480012|       5561.04| Mott St & Prince St|2023-10-01 01:12:04|     0|       casual|     false|07FF3D443D7DFA14| classic_bike|      40.70365182|      -74.01167797|         4962.08|Broad St & Bridge St|2023-09-30 23:08:30|         NULL|\n",
      "|      NULL|      40.72019576|      -73.98997825|       5414.06|Allen St & Riving...|2023-09-01 18:59:38|     0|       casual|     false|E32800B9B7C5E57F| classic_bike|     40.727082372|     -73.953095317|         5666.04|Meserole Ave & Ma...|2023-09-01 18:42:41|         NULL|\n",
      "|      NULL|40.72970805644994| -73.9865979552269|       5746.02|     E 10 St & 2 Ave|2023-09-30 14:48:17|     0|       casual|     false|A918CFF9F19BF77B| classic_bike|40.72486122254819|-73.99213135242462|         5636.13|     E 1 St & Bowery|2023-09-30 14:42:10|         NULL|\n",
      "|      NULL|      40.72019576|      -73.98997825|       5414.06|Allen St & Riving...|2023-09-27 11:14:47|     0|       casual|     false|0118161E3712C001| classic_bike|      40.70706456|      -74.00731853|         5065.10|Maiden Ln & Pearl St|2023-09-27 10:55:45|         NULL|\n",
      "|      NULL| 40.6999974843885|-73.97440128028393|       4843.01|    5 St & Market St|2023-09-28 14:04:32|     0|       casual|     false|04D1BCDAD563134D| classic_bike|     40.703657389|     -74.011291027|         4962.08|Broad St & Bridge St|2023-09-28 13:43:03|         NULL|\n",
      "|      NULL|      40.72019576|      -73.98997825|       5414.06|Allen St & Riving...|2023-09-12 01:34:07|     0|       casual|     false|A8A59F0CFE2B3412| classic_bike|40.72486122254819|-73.99213135242462|         5636.13|     E 1 St & Bowery|2023-09-12 01:18:36|         NULL|\n",
      "|      NULL|40.72970805644994| -73.9865979552269|       5746.02|     E 10 St & 2 Ave|2023-09-21 09:10:34|     0|       member|     false|D70D0A2E3A3F5905| classic_bike|       40.7500727|      -73.99839279|         6416.06|     W 29 St & 9 Ave|2023-09-21 08:52:45|         NULL|\n",
      "|      NULL|      40.75323098|      -73.97032517|       6498.10|     E 47 St & 2 Ave|2023-09-05 08:42:51|     0|       member|     false|EEB5E59F8FFF7AA9| classic_bike|40.76153045158114|-73.99007007479668|         6786.02|     W 47 St & 9 Ave|2023-09-05 08:33:32|         NULL|\n",
      "+----------+-----------------+------------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read.parquet(os.path.join(\"computed\", \"precompiled\", \"part_0023.parquet\"))\n",
    "df = spark.read.parquet(os.path.join(\"computed\", \"precompiled\", \"*.parquet\"))\n",
    "print(df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date_pre_covid = '2020-02-29'\n",
    "start_date_post_covid = '2020-03-01'\n",
    "\n",
    "df_pre_covid = df.filter(col('ended_at') <= end_date_pre_covid)\n",
    "df_2018 = df.filter((col('ended_at') <= '2019-01-01') & (col('started_at') > '2018-01-01')) \n",
    "\n",
    "df_post_covid = df.filter(col('started_at') >= start_date_post_covid)\n",
    "df_2022 = df.filter((col('ended_at') <= '2023-01-01') & (col('started_at') > '2022-01-01')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Covid-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+--------------+--------------------+--------------------+------+-------------+----------+-------+-------------+------------------+------------------+----------------+--------------------+--------------------+-------------+\n",
      "|birth_year|    end_lat|     end_lng|end_station_id|    end_station_name|            ended_at|gender|member_casual|old_format|ride_id|rideable_type|         start_lat|         start_lng|start_station_id|  start_station_name|          started_at|trip_duration|\n",
      "+----------+-----------+------------+--------------+--------------------+--------------------+------+-------------+----------+-------+-------------+------------------+------------------+----------------+--------------------+--------------------+-------------+\n",
      "|      1993|40.71850211|-73.98329859|         349.0|Rivington St & Ri...|2018-12-20 14:12:...|     1|       member|      true|  34649|         NULL|       40.72580614|      -73.97422494|           339.0|  Avenue D & E 12 St|2018-12-20 14:01:...|          642|\n",
      "|      1984| 40.7937704|  -73.971888|        3314.0|  W 95 St & Broadway|2018-12-20 14:06:...|     2|       member|      true|  27153|         NULL|          40.78839|          -73.9747|          3285.0|W 87 St & Amsterd...|2018-12-20 14:01:...|          285|\n",
      "|      1953| 40.6769694|   -73.96579|        3574.0|Prospect Pl & Und...|2018-12-20 14:12:...|     2|       member|      true|  17947|         NULL|40.675146838709786|-73.97523209452629|          3346.0| Berkeley Pl & 7 Ave|2018-12-20 14:01:...|          619|\n",
      "|      1988|40.71605866|-73.99190759|         361.0|Allen St & Hester St|2018-12-20 14:14:...|     1|       member|      true|  29757|         NULL|       40.72710258|      -74.00297088|           128.0|MacDougal St & Pr...|2018-12-20 14:01:...|          761|\n",
      "|      1961|40.74691959|-74.00451887|         462.0|    W 22 St & 10 Ave|2018-12-20 14:06:...|     1|       member|      true|  33800|         NULL|       40.74345335|      -74.00004031|           470.0|     W 20 St & 8 Ave|2018-12-20 14:01:...|          297|\n",
      "+----------+-----------+------------+--------------+--------------------+--------------------+------+-------------+----------+-------+-------------+------------------+------------------+----------------+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Covid-19\")\n",
    "df_pre_covid.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Covid-19\n",
      "+----------+-----------------+-----------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+\n",
      "|birth_year|          end_lat|          end_lng|end_station_id|    end_station_name|           ended_at|gender|member_casual|old_format|         ride_id|rideable_type|        start_lat|         start_lng|start_station_id|  start_station_name|         started_at|trip_duration|\n",
      "+----------+-----------------+-----------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+\n",
      "|      NULL|40.72970805644994|-73.9865979552269|       5746.02|     E 10 St & 2 Ave|2023-09-03 10:24:16|     0|       member|     false|B0A0F1DEFA4B72FC|electric_bike|40.72486122254819|-73.99213135242462|         5636.13|     E 1 St & Bowery|2023-09-03 10:20:41|         NULL|\n",
      "|      NULL|      40.72019576|     -73.98997825|       5414.06|Allen St & Riving...|2023-09-27 15:53:25|     0|       member|     false|2B26AB15647BF4EE| classic_bike|     40.704650044|     -74.009132624|         4993.02|Pearl St & Hanove...|2023-09-27 15:44:23|         NULL|\n",
      "|      NULL|40.72970805644994|-73.9865979552269|       5746.02|     E 10 St & 2 Ave|2023-09-19 13:48:11|     0|       member|     false|9D2B5971CA4E513F| classic_bike|     40.724753261|     -73.992115974|         5636.13|     E 1 St & Bowery|2023-09-19 13:40:48|         NULL|\n",
      "|      NULL|      40.72317958|     -73.99480012|       5561.04| Mott St & Prince St|2023-09-30 16:56:35|     0|       member|     false|17E6760596DC3ABE| classic_bike|     40.696706414|     -73.922934651|         4713.01|Central Ave & Him...|2023-09-30 16:27:50|         NULL|\n",
      "|      NULL|      40.72317958|     -73.99480012|       5561.04| Mott St & Prince St|2023-09-21 17:07:36|     0|       member|     false|97EFF376A7E2DC70| classic_bike|       40.7284186|      -73.98713956|         5669.10| St Marks Pl & 2 Ave|2023-09-21 16:59:53|         NULL|\n",
      "+----------+-----------------+-----------------+--------------+--------------------+-------------------+------+-------------+----------+----------------+-------------+-----------------+------------------+----------------+--------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"After Covid-19\")\n",
    "df_post_covid.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip distance by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Rayon de la Terre en kilomÃ¨tres\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trip_distance_over_week(df, verbose=False):\n",
    "    if verbose : print(\"compute distance\")\n",
    "\n",
    "    df = df.filter(\n",
    "        col(\"start_lat\").isNotNull() & \n",
    "        col(\"start_lng\").isNotNull() & \n",
    "        col(\"end_lat\").isNotNull() & \n",
    "        col(\"end_lng\").isNotNull()\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"distance_km\", haversine_udf(col(\"start_lat\"), \n",
    "                                     col(\"start_lng\"), \n",
    "                                     col(\"end_lat\"), \n",
    "                                     col(\"end_lng\")))\n",
    "    \n",
    "    if verbose : print(\"compute day of week\")\n",
    "\n",
    "    df = df.withColumn(\"day_of_week\", dayofweek(col(\"started_at\")))\n",
    "\n",
    "    if verbose : print(\"group by\")\n",
    "    \n",
    "    avg_distance_by_day_of_week = df.groupBy(\"day_of_week\") \\\n",
    "        .agg(avg(\"distance_km\").alias(\"avg_distance_km\"))\n",
    "    \n",
    "    avg_distance_by_day_of_week = avg_distance_by_day_of_week.withColumn(\n",
    "        \"day_of_week\",\n",
    "        when(col(\"day_of_week\") == 1, \"Sunday\")\n",
    "        .when(col(\"day_of_week\") == 2, \"Monday\")\n",
    "        .when(col(\"day_of_week\") == 3, \"Tuesday\")\n",
    "        .when(col(\"day_of_week\") == 4, \"Wednesday\")\n",
    "        .when(col(\"day_of_week\") == 5, \"Thursday\")\n",
    "        .when(col(\"day_of_week\") == 6, \"Friday\")\n",
    "        .when(col(\"day_of_week\") == 7, \"Saturday\")\n",
    "    )\n",
    "\n",
    "    return avg_distance_by_day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip distance over week in 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=====================================================>(114 + 1) / 115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|day_of_week|   avg_distance_km|\n",
      "+-----------+------------------+\n",
      "|     Sunday|1.7461603350028398|\n",
      "|     Friday|1.7766569111752966|\n",
      "|    Tuesday|1.7983723666188383|\n",
      "|   Thursday|  1.79577718680504|\n",
      "|  Wednesday|1.8015392269836976|\n",
      "|   Saturday|1.7828920533192332|\n",
      "|     Monday|1.7913233909698618|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Trip distance over week in 2018\")\n",
    "compute_trip_distance_over_week(df_2018).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip distance over week in 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:=====================================================>(113 + 2) / 115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|day_of_week|   avg_distance_km|\n",
      "+-----------+------------------+\n",
      "|     Sunday| 1.875066031662407|\n",
      "|     Friday|1.8119837277899957|\n",
      "|    Tuesday|1.8201703663119915|\n",
      "|   Thursday|1.8254055881989868|\n",
      "|  Wednesday|1.8318979345853759|\n",
      "|   Saturday|1.9080280784761932|\n",
      "|     Monday|1.7956361630068163|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Trip distance over week in 2022\")\n",
    "compute_trip_distance_over_week(df_2022).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of trips for each pickup/dropoff location couple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute trip distance distribution for gender (when available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute trip distance distribution for age ranges (15-24,25-44, 45-54, 55-64, 65+)\n",
    "(when available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute trip distance distribution for different kind of bikes (when available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Informations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
